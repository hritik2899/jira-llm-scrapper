{"metadata": {"id": "13632470", "key": "SPARK-54031", "title": "Add golden files for Analyzer edge cases", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Vladimir Golubev", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-25T21:20:48.000+0000", "updated": "2025-10-26T11:54:48.000+0000"}, "content": {"description": "New golden files for edge-cases discovered during the Analyzer support and development.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54031?", "answer": "New golden files for edge-cases discovered during the Analyzer support and development."}}}
{"metadata": {"id": "13632457", "key": "SPARK-54030", "title": "Add detailed error message for corrupted view metadata with mismatched column counts", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Ganesha S", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-25T16:11:19.000+0000", "updated": "2025-10-25T16:16:29.000+0000"}, "content": {"description": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54030?", "answer": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch."}}}
{"metadata": {"id": "13632454", "key": "SPARK-54029", "title": "Display a detailed error message when table metadata is corrupted", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Ganesha S", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-25T14:57:47.000+0000", "updated": "2025-10-25T15:04:25.000+0000"}, "content": {"description": "Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54029?", "answer": "Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation."}}}
{"metadata": {"id": "13632452", "key": "SPARK-54028", "title": "Use empty schema when altering a view which is not Hive compatible", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Chiran Ravani", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-25T12:47:26.000+0000", "updated": "2025-10-25T12:57:26.000+0000"}, "content": {"description": "Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.", "comments": ["I am working on a Fix, Can I assign to myself?"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54028?", "answer": "Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation."}}}
{"metadata": {"id": "13632438", "key": "SPARK-54027", "title": "Kafka Source RTM support", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-25T00:06:42.000+0000", "updated": "2025-10-25T00:16:00.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54027?", "answer": "No description available."}}}
{"metadata": {"id": "13632437", "key": "SPARK-54026", "title": "Use BOM for AWS Java SDK V2 dependency management", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Vlad Rozov", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-24T23:39:36.000+0000", "updated": "2025-10-25T00:06:53.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54026?", "answer": "No description available."}}}
{"metadata": {"id": "13632429", "key": "SPARK-54025", "title": "Support recaching when a table is written via a different table implementation (V1 or V2)", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-24T20:25:04.000+0000", "updated": "2025-10-24T20:25:31.000+0000"}, "content": {"description": "When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed.\r\n\r\n \r\n\r\nThis issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice.\r\n\r\n \r\n\r\n*Proposed Fix:*\r\n\r\nEnhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 ↔ V2) and trigger recaching accordingly.\r\n\r\n \r\n\r\n{*}Expected Outcome:{*}{*}{*}\r\n * Cached data remains up to date when a table is written through either V1 or V2 paths.\r\n\r\n * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 connectors\r\n\r\n ", "comments": ["cc [~vli-databricks]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54025?", "answer": "When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed.\r\n\r\n \r\n\r\nThis issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice.\r\n\r\n \r\n\r\n*Proposed Fix:*\r\n\r\nEnhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 ↔ V2) and trigger recaching accordingly.\r\n\r\n \r\n\r\n{*}Expected Outcome:{*}{*}{*}\r\n * Cached data remains up to date when a table is written through either V1 or V2 paths.\r\n\r\n * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 connectors\r\n\r\n "}}}
{"metadata": {"id": "13632428", "key": "SPARK-54024", "title": "add sbt-dependency-graph to SBT plugins", "project": "SPARK", "status": "Open", "priority": "Minor", "reporter": "Vlad Rozov", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-24T19:52:00.000+0000", "updated": "2025-10-24T20:03:06.000+0000"}, "content": {"description": "The plugin adds few useful commands to browser dependencies tree in SBT.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54024?", "answer": "The plugin adds few useful commands to browser dependencies tree in SBT."}}}
{"metadata": {"id": "13632426", "key": "SPARK-54023", "title": "Support `AUTO` Netty IO Mode", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-24T18:42:53.000+0000", "updated": "2025-10-26T10:03:34.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52724\n[https://github.com/apache/spark/pull/52724]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54023?", "answer": "No description available."}}}
{"metadata": {"id": "13632421", "key": "SPARK-54022", "title": "Make DSv2 table resolution aware of cached tables", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "labels": [], "created": "2025-10-24T17:40:06.000+0000", "updated": "2025-10-24T17:40:06.000+0000"}, "content": {"description": "Make DSv2 table resolution aware of cached tables. If the user executes CACHE TABLE t, the subsequent resolution calls must use the cached table metadata, instead of potentially allowing the connectors to refresh and break the cache lookup.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54022?", "answer": "Make DSv2 table resolution aware of cached tables. If the user executes CACHE TABLE t, the subsequent resolution calls must use the cached table metadata, instead of potentially allowing the connectors to refresh and break the cache lookup."}}}
{"metadata": {"id": "13632420", "key": "SPARK-54021", "title": "Implement Geography and Geometry accessors across Catalyst", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-24T17:09:51.000+0000", "updated": "2025-10-24T23:29:44.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52723."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54021?", "answer": "No description available."}}}
{"metadata": {"id": "13632416", "key": "SPARK-54020", "title": "Support spark.sql(\"SELECT ...\") inside Pipelines query functions", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "labels": [], "created": "2025-10-24T16:22:23.000+0000", "updated": "2025-10-24T16:22:23.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54020?", "answer": "No description available."}}}
{"metadata": {"id": "13632415", "key": "SPARK-54019", "title": "Prevent spark.sql session state mutation within Python pipeline files", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "labels": [], "created": "2025-10-24T16:19:35.000+0000", "updated": "2025-10-24T16:19:35.000+0000"}, "content": {"description": "We should ban all uses of spark.sql except for select statements.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54019?", "answer": "We should ban all uses of spark.sql except for select statements."}}}
{"metadata": {"id": "13632414", "key": "SPARK-54018", "title": "Upgrade Volcano to 1.13.0", "project": "SPARK", "status": "In Progress", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-24T16:06:39.000+0000", "updated": "2025-10-24T16:11:26.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54018?", "answer": "No description available."}}}
{"metadata": {"id": "13632412", "key": "SPARK-54017", "title": "Audit test dependencies in Spark 4.1.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": [], "created": "2025-10-24T15:35:21.000+0000", "updated": "2025-10-24T15:46:38.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54017?", "answer": "No description available."}}}
{"metadata": {"id": "13632409", "key": "SPARK-54016", "title": "Improve K8s support in Spark 4.1.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["releasenotes"], "created": "2025-10-24T15:06:06.000+0000", "updated": "2025-10-24T15:29:24.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54016?", "answer": "No description available."}}}
{"metadata": {"id": "13632402", "key": "SPARK-54015", "title": "Relex Py4J requirement to 0.10.9.7+", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Hyukjin Kwon", "assignee": "Hyukjin Kwon", "labels": ["pull-request-available"], "created": "2025-10-24T12:54:21.000+0000", "updated": "2025-10-25T23:56:27.000+0000"}, "content": {"description": "JVM are compatible with  0.10.9.7+ and above versions have some correctness fixes", "comments": ["Issue resolved by pull request 52721\n[https://github.com/apache/spark/pull/52721]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54015?", "answer": "JVM are compatible with  0.10.9.7+ and above versions have some correctness fixes"}}}
{"metadata": {"id": "13632372", "key": "SPARK-54014", "title": "Support setMaxRows for SparkConnectStatement", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": [], "created": "2025-10-24T08:51:13.000+0000", "updated": "2025-10-24T08:51:13.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54014?", "answer": "No description available."}}}
{"metadata": {"id": "13632371", "key": "SPARK-54013", "title": "Implement SparkConnectDatabaseMetaData simple methods", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": [], "created": "2025-10-24T08:48:36.000+0000", "updated": "2025-10-24T08:51:24.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54013?", "answer": "No description available."}}}
{"metadata": {"id": "13632364", "key": "SPARK-54012", "title": "Improve Netty usage patterns", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Kent Yao", "labels": ["releasenotes"], "created": "2025-10-24T05:16:15.000+0000", "updated": "2025-10-24T15:02:44.000+0000"}, "content": {"description": "", "comments": ["Hi, [~yao]. As a recognition of your contribution, I made this umbrella JIRA issue with `releasenotes` label.\r\nAlthough I have more ideas for this topic and will add some, I believe we are able to mark this as a *resolved* item for Apache Spark 4.1.0 by containing only what we did and what we want to add until the feature freeze. WDYT?", "Thank you for making this umbrella, [~dongjoon]. Making it resolved sounds good to me", "Thank you, [~yao]."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54012?", "answer": "No description available."}}}
{"metadata": {"id": "13632358", "key": "SPARK-54011", "title": "Switch direct ctors to IoHandlerFactories for EventLoopGroups", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "labels": ["pull-request-available"], "created": "2025-10-24T03:03:56.000+0000", "updated": "2025-10-24T14:47:07.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52719\n[https://github.com/apache/spark/pull/52719]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54011?", "answer": "No description available."}}}
{"metadata": {"id": "13632344", "key": "SPARK-54010", "title": "Support restart counter in Spark app", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Zhou JIANG", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T23:50:13.000+0000", "updated": "2025-10-24T22:53:06.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54010?", "answer": "No description available."}}}
{"metadata": {"id": "13632341", "key": "SPARK-54009", "title": "Support `spark.io.mode.default`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-23T22:21:19.000+0000", "updated": "2025-10-24T13:45:50.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52717\n[https://github.com/apache/spark/pull/52717]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54009?", "answer": "No description available."}}}
{"metadata": {"id": "13632335", "key": "SPARK-54008", "title": "Skip query execution for DESCRIBE QUERY", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Gene Pang", "assignee": "Gene Pang", "labels": ["pull-request-available"], "created": "2025-10-23T20:11:47.000+0000", "updated": "2025-10-24T18:57:40.000+0000"}, "content": {"description": "DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command.", "comments": ["Issue resolved by pull request 52713\n[https://github.com/apache/spark/pull/52713]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54008?", "answer": "DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command."}}}
{"metadata": {"id": "13632332", "key": "SPARK-54007", "title": "Use Java `Set.of` instead of `Collections.emptySet`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-23T19:08:20.000+0000", "updated": "2025-10-24T04:59:46.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52711\n[https://github.com/apache/spark/pull/52711]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54007?", "answer": "No description available."}}}
{"metadata": {"id": "13632331", "key": "SPARK-54006", "title": "WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above", "project": "SPARK", "status": "Open", "priority": "Minor", "reporter": "Vlad Rozov", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T18:45:57.000+0000", "updated": "2025-10-24T15:45:26.000+0000"}, "content": {"description": "Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with\r\n{code}\r\nWithAggregationKinesisBackedBlockRDDSuite:\r\n*** RUN ABORTED ***\r\n  java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.<init>(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  at scala.collection.immutable.Range.foreach(Range.scala:158)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57)\r\n  at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134)\r\n  ...\r\n  Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter\r\n  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.<init>(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  ...\r\n{code}", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54006?", "answer": "Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with\r\n{code}\r\nWithAggregationKinesisBackedBlockRDDSuite:\r\n*** RUN ABORTED ***\r\n  java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.<init>(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  at scala.collection.immutable.Range.foreach(Range.scala:158)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57)\r\n  at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134)\r\n  ...\r\n  Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter\r\n  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.<init>(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  ...\r\n{code}"}}}
{"metadata": {"id": "13632329", "key": "SPARK-54005", "title": "Remove no-op `spark.shuffle.blockTransferService` configuration", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-23T18:14:08.000+0000", "updated": "2025-10-24T05:20:43.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52709\n[https://github.com/apache/spark/pull/52709]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54005?", "answer": "No description available."}}}
{"metadata": {"id": "13632315", "key": "SPARK-54004", "title": "Fix uncaching table by name without cascading", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": "Anton Okolnychyi", "labels": ["pull-request-available"], "created": "2025-10-23T14:45:28.000+0000", "updated": "2025-10-24T14:41:42.000+0000"}, "content": {"description": "{{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias.", "comments": ["Issue resolved by pull request 52712\n[https://github.com/apache/spark/pull/52712]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54004?", "answer": "{{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias."}}}
{"metadata": {"id": "13632302", "key": "SPARK-54003", "title": "Use the staging directory as the output path then move to final path", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Chenyu Zheng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T13:12:48.000+0000", "updated": "2025-10-24T07:18:14.000+0000"}, "content": {"description": "SparkSQL uses the partition location or table location as the commit path (except in *_dynamic partition overwrite_* mode and *_custom partition path_* mode). This has at least the following issues:\r\n\r\n* As described in SPARK-37210, conflicts can occur when multiple partitions job of the same table are run concurrently. Using a staging directory can avoid this issue.\r\n* As described in SPARK-53937, using a staging directory allows for near-atomic operations.\r\n\r\n_*Dynamic partition overwrite*_ mode and *_custom partition path_* mode already use the staging directory. And *_dynamic partition overwrite_* mode and _*custom partition path*_ are implemented differently, which can be further simplified into a unified process. And in https://github.com/apache/spark/pull/29000, reset the staging directory as the output directory of FileOutputCommitter. This way is more safer. It should be modified to this way.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54003?", "answer": "SparkSQL uses the partition location or table location as the commit path (except in *_dynamic partition overwrite_* mode and *_custom partition path_* mode). This has at least the following issues:\r\n\r\n* As described in SPARK-37210, conflicts can occur when multiple partitions job of the same table are run concurrently. Using a staging directory can avoid this issue.\r\n* As described in SPARK-53937, using a staging directory allows for near-atomic operations.\r\n\r\n_*Dynamic partition overwrite*_ mode and *_custom partition path_* mode already use the staging directory. And *_dynamic partition overwrite_* mode and _*custom partition path*_ are implemented differently, which can be further simplified into a unified process. And in https://github.com/apache/spark/pull/29000, reset the staging directory as the output directory of FileOutputCommitter. This way is more safer. It should be modified to this way."}}}
{"metadata": {"id": "13632298", "key": "SPARK-54002", "title": "Support integrating BeeLine with Connect JDBC driver", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T12:04:46.000+0000", "updated": "2025-10-24T14:56:51.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54002?", "answer": "No description available."}}}
{"metadata": {"id": "13632289", "key": "SPARK-54001", "title": "Reduce memory footprint from cached local relations upon cloning", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": "Pranav Dev", "labels": ["pull-request-available"], "created": "2025-10-23T09:38:20.000+0000", "updated": "2025-10-24T22:22:34.000+0000"}, "content": {"description": "Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when:\r\n * Sessions are cloned frequently\r\n * Cached relations contain large datasets\r\n * Multiple clones exist simultaneously\r\n\r\n \r\n\r\nAn improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations. ", "comments": ["Issue resolved by pull request 52651\n[https://github.com/apache/spark/pull/52651]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54001?", "answer": "Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when:\r\n * Sessions are cloned frequently\r\n * Cached relations contain large datasets\r\n * Multiple clones exist simultaneously\r\n\r\n \r\n\r\nAn improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations. "}}}
{"metadata": {"id": "13632285", "key": "SPARK-54000", "title": "Complex sql with expand operator and code gen enabled, very slow", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "lifulong", "assignee": null, "labels": [], "created": "2025-10-23T08:46:01.000+0000", "updated": "2025-10-23T08:51:49.000+0000"}, "content": {"description": "Complex sql with expand operator and code gen enabled, very slow\r\n\r\nsql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb\r\n\r\nwhen disable whole stage code gen, run will speed up 20x times\r\n\r\nwhen add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times\r\n\r\nreduce select column count, such as 28 -> 27, can speed up 10x times", "comments": ["!https://wiki.in.zhihu.com/download/attachments/640447372/image2025-10-2_13-32-26.png?version=1&modificationDate=1759383146774&api=v2!\r\n\r\nfrom flame graph we can see, most time cost is setNullAt call when enable whole stage code gen and not add -XX:-TieredCompilation jvm parameter"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-54000?", "answer": "Complex sql with expand operator and code gen enabled, very slow\r\n\r\nsql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb\r\n\r\nwhen disable whole stage code gen, run will speed up 20x times\r\n\r\nwhen add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times\r\n\r\nreduce select column count, such as 28 -> 27, can speed up 10x times"}}}
{"metadata": {"id": "13632280", "key": "SPARK-53999", "title": "Native KQueue Transport support on BSD/MacOS", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "labels": ["pull-request-available"], "created": "2025-10-23T07:27:17.000+0000", "updated": "2025-10-24T05:21:30.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52703\n[https://github.com/apache/spark/pull/52703]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53999?", "answer": "No description available."}}}
{"metadata": {"id": "13632275", "key": "SPARK-53998", "title": "Add addition E2E tests for RTM", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T04:51:16.000+0000", "updated": "2025-10-24T05:09:52.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53998?", "answer": "No description available."}}}
{"metadata": {"id": "13632270", "key": "SPARK-53997", "title": "Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "labels": [], "created": "2025-10-23T03:14:09.000+0000", "updated": "2025-10-23T03:15:02.000+0000"}, "content": {"description": "When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling.", "comments": ["PR: https://github.com/apache/spark/pull/52685"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53997?", "answer": "When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling."}}}
{"metadata": {"id": "13632260", "key": "SPARK-53996", "title": "InferFiltersFromConstraints rule does not infer filter conditions for complex join expressions", "project": "SPARK", "status": "Open", "priority": "Minor", "reporter": "Jaromir Vanek", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-23T00:29:21.000+0000", "updated": "2025-10-23T01:14:51.000+0000"}, "content": {"description": "The Spark optimizer's {{InferFiltersFromConstraints}} rule is currently unable to infer filter conditions when join constraints involve complex expressions. While it works for simple attribute equalities ({{{}a = b{}}}), it can't infer constraint from anything more complex than that.\r\n\r\n*Example (works as expected):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN t2 ON t1.a = t2.b\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, the optimizer correctly infers the additional constraint {{{}t1.a = 1{}}}.\r\n\r\n*Example (does not work):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN right ON t1.a = t2.b + 2\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, it is clear that {{t1.a = 3}} (since {{b = 1}} and {{{}a = b + 2{}}}), but the optimizer does not infer this constraint.\r\n\r\n*How to Reproduce:*\r\n{code:scala}\r\nspark.sql(\"CREATE TABLE t1(a INT)\")\r\nspark.sql(\"CREATE TABLE t2(b INT)\")\r\n\r\nspark.sql(\"\"\"\r\nSELECT * \r\nFROM t1 \r\nINNER JOIN t2 ON t2.b = t1.a + 2 \r\nWHERE t1.a = 1\r\n\"\"\").explain\r\n{code}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan\r\n+- BroadcastHashJoin [(a#2 + 2)], [b#3], Inner, BuildRight, false\r\n   :- Filter (isnotnull(a#2) AND (a#2 = 1))\r\n   :  +- FileScan spark_catalog.default.t1[a#2]\r\n      +- Filter isnotnull(b#3)\r\n         +- FileScan spark_catalog.default.t2[b#3]\r\n{code}\r\n*Expected Behavior:*\r\nThe optimizer should be able to statically evaluate and infer that {{t2.b = 3}} given the join condition and the filter on {{{}t1.a{}}}.\r\n\r\n*Impact:*\r\nThis limits the optimizer's ability to push down filters and optimize query execution plans for queries with complex join conditions.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53996?", "answer": "The Spark optimizer's {{InferFiltersFromConstraints}} rule is currently unable to infer filter conditions when join constraints involve complex expressions. While it works for simple attribute equalities ({{{}a = b{}}}), it can't infer constraint from anything more complex than that.\r\n\r\n*Example (works as expected):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN t2 ON t1.a = t2.b\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, the optimizer correctly infers the additional constraint {{{}t1.a = 1{}}}.\r\n\r\n*Example (does not work):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN right ON t1.a = t2.b + 2\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, it is clear that {{t1.a = 3}} (since {{b = 1}} and {{{}a = b + 2{}}}), but the optimizer does not infer this constraint.\r\n\r\n*How to Reproduce:*\r\n{code:scala}\r\nspark.sql(\"CREATE TABLE t1(a INT)\")\r\nspark.sql(\"CREATE TABLE t2(b INT)\")\r\n\r\nspark.sql(\"\"\"\r\nSELECT * \r\nFROM t1 \r\nINNER JOIN t2 ON t2.b = t1.a + 2 \r\nWHERE t1.a = 1\r\n\"\"\").explain\r\n{code}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan\r\n+- BroadcastHashJoin [(a#2 + 2)], [b#3], Inner, BuildRight, false\r\n   :- Filter (isnotnull(a#2) AND (a#2 = 1))\r\n   :  +- FileScan spark_catalog.default.t1[a#2]\r\n      +- Filter isnotnull(b#3)\r\n         +- FileScan spark_catalog.default.t2[b#3]\r\n{code}\r\n*Expected Behavior:*\r\nThe optimizer should be able to statically evaluate and infer that {{t2.b = 3}} given the join condition and the filter on {{{}t1.a{}}}.\r\n\r\n*Impact:*\r\nThis limits the optimizer's ability to push down filters and optimize query execution plans for queries with complex join conditions."}}}
{"metadata": {"id": "13632257", "key": "SPARK-53995", "title": "Add support to track `spark_conf` at the dataset level", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Yuheng Chang", "assignee": null, "labels": [], "created": "2025-10-22T23:30:17.000+0000", "updated": "2025-10-24T05:10:07.000+0000"}, "content": {"description": "We need to add {{sql_confs}} to {{DefineOutput}} in SDP to support following use case\r\n\r\nSQL file #1\r\n{code:java}\r\nSET some_conf = some_value;\r\nCREATE STREAMING TABLE st; {code}\r\nSQL file #2\r\nCREATE FLOW INSERT INTO st FROM ....\r\n \r\nSince we don't store {{sql_confs}} at the Dataset level, the conf would be ignored. We should store it at dataset level and will merge the {{sql_conf}} stored at the dataset level to its corresponding flow when we construct the DataflowGraph.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53995?", "answer": "We need to add {{sql_confs}} to {{DefineOutput}} in SDP to support following use case\r\n\r\nSQL file #1\r\n{code:java}\r\nSET some_conf = some_value;\r\nCREATE STREAMING TABLE st; {code}\r\nSQL file #2\r\nCREATE FLOW INSERT INTO st FROM ....\r\n \r\nSince we don't store {{sql_confs}} at the Dataset level, the conf would be ignored. We should store it at dataset level and will merge the {{sql_conf}} stored at the dataset level to its corresponding flow when we construct the DataflowGraph."}}}
{"metadata": {"id": "13632255", "key": "SPARK-53994", "title": "Exclude `KerberosConfDriverFeatureStep` during benchmarking", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T22:22:00.000+0000", "updated": "2025-10-22T23:16:50.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 404\n[https://github.com/apache/spark-kubernetes-operator/pull/404]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53994?", "answer": "No description available."}}}
{"metadata": {"id": "13632253", "key": "SPARK-53993", "title": "Support `spark.logConf` configuration", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T22:00:18.000+0000", "updated": "2025-10-22T22:12:10.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 403\n[https://github.com/apache/spark-kubernetes-operator/pull/403]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53993?", "answer": "No description available."}}}
{"metadata": {"id": "13632251", "key": "SPARK-53992", "title": "Add `SparkOperatorConfManager.getAll` method", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T21:15:34.000+0000", "updated": "2025-10-22T21:35:31.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 402\n[https://github.com/apache/spark-kubernetes-operator/pull/402]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53992?", "answer": "No description available."}}}
{"metadata": {"id": "13632250", "key": "SPARK-53991", "title": "Add support for KLL quantiles functions based on DataSketches", "project": "SPARK", "status": "In Progress", "priority": "Major", "reporter": "Daniel Tenedorio", "assignee": "Daniel Tenedorio", "labels": [], "created": "2025-10-22T20:42:35.000+0000", "updated": "2025-10-24T05:10:22.000+0000"}, "content": {"description": "Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]", "comments": ["We can use the following function names. For each category, we can support long integer, single-precision floating-point, and double-precision floating-point variants (it is necessary to specify which one we are using since the representation of the sketch buffers is different for different input value data types).\r\n\r\n \r\n\r\nAggregate functions to consume input values and return a sketch buffer:\r\n\r\n{{kll_sketch_agg_bigint(col)}}\r\n\r\n{{kll_sketch_agg_float(col)}}\r\n\r\n{{kll_sketch_agg_double(col)}}\r\n\r\n \r\n\r\nScalar functions to merge two sketch buffers together into another sketch buffer:\r\n\r\n{{kll_sketch_merge_bigint(sketch1, sketch2)}}\r\n\r\n{{kll_sketch_merge_float(sketch1, sketch2)}}\r\n\r\n{{kll_sketch_merge_double(sketch1, sketch2)}}\r\n\r\n \r\n\r\nScalar functions to extract a single value from the quantiles sketch representing the desired quantile given the input rank (for example, \"float median = sketch.getQuantile(0.5)\"). We can also implement the functions in this category to also support accepting an array of input ranks and return an array of result quantiles.\r\n\r\n{{kll_sketch_get_quantile_bigint(sketch, rank)}}\r\n\r\n{{kll_sketch_get_quantile_float(sketch, rank)}}\r\n\r\n{{kll_sketch_get_quantile_double(sketch, rank)}}\r\n\r\n \r\n\r\nScalar functions to extract a single value from the quantiles sketch representing the desired rank given the input quantile (for example, \"double rankOf1000 = sketch.getRank(1000)\"). We can also implement the functions in this category to also support accepting an array of input quantiles and return an array of result ranks.\r\n\r\n{{kll_sketch_get_rank_bigint(sketch, quantile)}}\r\n\r\n{{kll_sketch_get_rank_float(sketch, quantile)}}\r\n\r\n{{kll_sketch_get_rank_double(sketch, quantile)}}\r\n\r\n \r\n\r\nOptional, scalar functions to return a string representation of a sketch buffer:\r\n\r\n{{kll_sketch_to_string_bigint(sketch)}}\r\n\r\n{{kll_sketch_to_string_float(sketch)}}\r\n\r\n{{kll_sketch_to_string_double(sketch)}}"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53991?", "answer": "Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]"}}}
{"metadata": {"id": "13632249", "key": "SPARK-53990", "title": "Use Java `(Map|Set).of` instead of `Collections.(empty|singleton)(Set|Map)`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T20:36:22.000+0000", "updated": "2025-10-22T21:08:57.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 401\n[https://github.com/apache/spark-kubernetes-operator/pull/401]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53990?", "answer": "No description available."}}}
{"metadata": {"id": "13632243", "key": "SPARK-53989", "title": "Optimize `sparkapps.sh` to use `kubectl delete --all`", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T19:31:05.000+0000", "updated": "2025-10-22T20:00:43.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 400\n[https://github.com/apache/spark-kubernetes-operator/pull/400]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53989?", "answer": "No description available."}}}
{"metadata": {"id": "13632242", "key": "SPARK-53988", "title": "Upgrade `Spotless` to 8.0.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T18:52:41.000+0000", "updated": "2025-10-22T19:59:52.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 399\n[https://github.com/apache/spark-kubernetes-operator/pull/399]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53988?", "answer": "No description available."}}}
{"metadata": {"id": "13632240", "key": "SPARK-53987", "title": "Upgrade `JaCoCo` to 0.8.14 for official Java 25 support", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T18:37:18.000+0000", "updated": "2025-10-22T18:51:17.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 398\n[https://github.com/apache/spark-kubernetes-operator/pull/398]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53987?", "answer": "No description available."}}}
{"metadata": {"id": "13632239", "key": "SPARK-53986", "title": "Reorganize Python streaming TWS test", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Huanli Wang", "assignee": "Huanli Wang", "labels": ["pull-request-available"], "created": "2025-10-22T18:36:07.000+0000", "updated": "2025-10-24T22:19:51.000+0000"}, "content": {"description": "Reorganize the Python TWS tests:\r\n* moving tws related tests to a new `/streaming` directory\r\n\r\n* further split the Python TWS tests to smaller ones to speed up the CI", "comments": ["Issue resolved by pull request 52691\n[https://github.com/apache/spark/pull/52691]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53986?", "answer": "Reorganize the Python TWS tests:\r\n* moving tws related tests to a new `/streaming` directory\r\n\r\n* further split the Python TWS tests to smaller ones to speed up the CI"}}}
{"metadata": {"id": "13632231", "key": "SPARK-53985", "title": "Suppress `StatusRecorder` warning messages", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T17:53:28.000+0000", "updated": "2025-10-22T18:06:47.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 397\n[https://github.com/apache/spark-kubernetes-operator/pull/397]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53985?", "answer": "No description available."}}}
{"metadata": {"id": "13632230", "key": "SPARK-53984", "title": "Use CRD `v1` instead of `v1beta1` in `benchmark` script", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T17:44:13.000+0000", "updated": "2025-10-22T18:00:26.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 396\n[https://github.com/apache/spark-kubernetes-operator/pull/396]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53984?", "answer": "No description available."}}}
{"metadata": {"id": "13632229", "key": "SPARK-53983", "title": "Increase `s.k.o.reconciler.foregroundRequestTimeoutSeconds` to `60s`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-22T17:39:42.000+0000", "updated": "2025-10-22T17:59:45.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 395\n[https://github.com/apache/spark-kubernetes-operator/pull/395]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53983?", "answer": "No description available."}}}
{"metadata": {"id": "13632223", "key": "SPARK-53982", "title": "Spark aggregation is incorrect (floating point error)", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Ian Manning", "assignee": null, "labels": [], "created": "2025-10-22T15:33:34.000+0000", "updated": "2025-10-22T15:35:49.000+0000"}, "content": {"description": " \r\n{code:java}\r\nList<Row> data = Arrays.asList(\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95)\r\n);\r\n \r\nStructType schema = DataTypes.createStructType(new StructField[] {\r\nDataTypes.createStructField(\"timestamp\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"id\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"value\", DataTypes.DoubleType, false)\r\n});\r\n \r\nDataset<Row> df = spark.createDataFrame(data, schema);\r\n \r\n// Show the input data\r\nSystem.out.println(\"Input data:\");\r\ndf.show();\r\n \r\n// Perform the aggregation\r\nDataset<Row> result = df.groupBy(\"id\")\r\n.agg(\r\navg(\"value\").as(METADATA_COL_METRICVALUE),\r\nsum(\"value\").as(METADATA_COL_SUM_VALUE)\r\n);\r\n \r\n// Show the results\r\nSystem.out.println(\"Aggregation results:\");\r\nresult.show();\r\n \r\n// Collect the results\r\nList<Row> results = result.collectAsList();\r\n \r\n// Print the results\r\nSystem.out.println(\"Number of results: \" + results.size());\r\nfor (Row row : results) {\r\nSystem.out.println(\"Metric value: \" + row.getDouble(row.fieldIndex(METADATA_COL_METRICVALUE)));\r\nSystem.out.println(\"Sum value: \" + row.getDouble(row.fieldIndex(METADATA_COL_SUM_VALUE)));\r\n}\r\n \r\n// Verify the results\r\nassertEquals(1, results.size(), \"Expected 1 aggregated result\");\r\n \r\nRow resultRow = results.get(0);\r\ndoublesumValue = resultRow.getDouble(resultRow.fieldIndex(METADATA_COL_SUM_VALUE));\r\ndoubleexpectedSum = 799.6; // 8 * 99.95\r\n \r\nSystem.out.println(\"Expected sum: \" + expectedSum);\r\nSystem.out.println(\"Actual sum: \" + sumValue);\r\nSystem.out.println(\"Difference: \" + Math.abs(expectedSum - sumValue));\r\n \r\n// Check if the sum is close to the expected value\r\nassertTrue(Math.abs(expectedSum - sumValue) < 0.001,\r\n\"Sum value should be close to \" + expectedSum + \" but was \" + sumValue);\r\n{code}\r\n\r\n\r\n\r\n{code:java}\r\nInput data:\r\n+--------------------+------+-----+\r\n|           timestamp|    id|value|\r\n+--------------------+------+-----+\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n+--------------------+------+-----+\r\n\r\nAggregation results:\r\n+------+-----------------+-----------------+\r\n|    id|     metric_value|        sum_value|\r\n+------+-----------------+-----------------+\r\n|pod123|99.95000000000002|799.6000000000001|\r\n+------+-----------------+-----------------+\r\n\r\nNumber of results: 1\r\nMetric value: 99.95000000000002\r\nSum value: 799.6000000000001\r\nExpected sum: 799.6\r\nActual sum: 799.6000000000001\r\nDifference: 1.1368683772161603E-13{code}", "comments": ["This does work with DecimalType - but I am not sure why it should not work with DoubleType given the level of precision of 2 digits."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53982?", "answer": " \r\n{code:java}\r\nList<Row> data = Arrays.asList(\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95)\r\n);\r\n \r\nStructType schema = DataTypes.createStructType(new StructField[] {\r\nDataTypes.createStructField(\"timestamp\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"id\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"value\", DataTypes.DoubleType, false)\r\n});\r\n \r\nDataset<Row> df = spark.createDataFrame(data, schema);\r\n \r\n// Show the input data\r\nSystem.out.println(\"Input data:\");\r\ndf.show();\r\n \r\n// Perform the aggregation\r\nDataset<Row> result = df.groupBy(\"id\")\r\n.agg(\r\navg(\"value\").as(METADATA_COL_METRICVALUE),\r\nsum(\"value\").as(METADATA_COL_SUM_VALUE)\r\n);\r\n \r\n// Show the results\r\nSystem.out.println(\"Aggregation results:\");\r\nresult.show();\r\n \r\n// Collect the results\r\nList<Row> results = result.collectAsList();\r\n \r\n// Print the results\r\nSystem.out.println(\"Number of results: \" + results.size());\r\nfor (Row row : results) {\r\nSystem.out.println(\"Metric value: \" + row.getDouble(row.fieldIndex(METADATA_COL_METRICVALUE)));\r\nSystem.out.println(\"Sum value: \" + row.getDouble(row.fieldIndex(METADATA_COL_SUM_VALUE)));\r\n}\r\n \r\n// Verify the results\r\nassertEquals(1, results.size(), \"Expected 1 aggregated result\");\r\n \r\nRow resultRow = results.get(0);\r\ndoublesumValue = resultRow.getDouble(resultRow.fieldIndex(METADATA_COL_SUM_VALUE));\r\ndoubleexpectedSum = 799.6; // 8 * 99.95\r\n \r\nSystem.out.println(\"Expected sum: \" + expectedSum);\r\nSystem.out.println(\"Actual sum: \" + sumValue);\r\nSystem.out.println(\"Difference: \" + Math.abs(expectedSum - sumValue));\r\n \r\n// Check if the sum is close to the expected value\r\nassertTrue(Math.abs(expectedSum - sumValue) < 0.001,\r\n\"Sum value should be close to \" + expectedSum + \" but was \" + sumValue);\r\n{code}\r\n\r\n\r\n\r\n{code:java}\r\nInput data:\r\n+--------------------+------+-----+\r\n|           timestamp|    id|value|\r\n+--------------------+------+-----+\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n|2021-01-01T00:00:...|pod123|99.95|\r\n+--------------------+------+-----+\r\n\r\nAggregation results:\r\n+------+-----------------+-----------------+\r\n|    id|     metric_value|        sum_value|\r\n+------+-----------------+-----------------+\r\n|pod123|99.95000000000002|799.6000000000001|\r\n+------+-----------------+-----------------+\r\n\r\nNumber of results: 1\r\nMetric value: 99.95000000000002\r\nSum value: 799.6000000000001\r\nExpected sum: 799.6\r\nActual sum: 799.6000000000001\r\nDifference: 1.1368683772161603E-13{code}"}}}
{"metadata": {"id": "13632187", "key": "SPARK-53981", "title": "Upgrade Netty to 4.2.7.Final", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "labels": ["pull-request-available"], "created": "2025-10-22T09:52:49.000+0000", "updated": "2025-10-24T05:19:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52695\n[https://github.com/apache/spark/pull/52695]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53981?", "answer": "No description available."}}}
{"metadata": {"id": "13632176", "key": "SPARK-53980", "title": "Add `SparkConf.getAllWithPrefix(String, String => K)` API", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jiaan Geng", "assignee": "Jiaan Geng", "labels": ["pull-request-available"], "created": "2025-10-22T07:36:01.000+0000", "updated": "2025-10-24T21:22:08.000+0000"}, "content": {"description": "We need to set some config related to S3 for our inner Spark. The implementation of the function show below.\r\n\r\n{code:java}\r\nprivate def setS3Configs(conf: SparkConf): Unit = {\r\n    val S3A_PREFIX = \"spark.fs.s3a\"\r\n    val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\"\r\n    val s3aConf = conf.getAllWithPrefix(S3A_PREFIX)\r\n    s3aConf\r\n      .foreach(\r\n        confPair => {\r\n          val keyWithoutPrefix = confPair._1\r\n          val oldKey = S3A_PREFIX + keyWithoutPrefix\r\n          val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix\r\n          val value = confPair._2\r\n          (newKey, value)\r\n        })\r\n  }\r\n\r\n{code}\r\n\r\nThese code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.", "comments": ["Issue resolved by pull request 52693\n[https://github.com/apache/spark/pull/52693]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53980?", "answer": "We need to set some config related to S3 for our inner Spark. The implementation of the function show below.\r\n\r\n{code:java}\r\nprivate def setS3Configs(conf: SparkConf): Unit = {\r\n    val S3A_PREFIX = \"spark.fs.s3a\"\r\n    val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\"\r\n    val s3aConf = conf.getAllWithPrefix(S3A_PREFIX)\r\n    s3aConf\r\n      .foreach(\r\n        confPair => {\r\n          val keyWithoutPrefix = confPair._1\r\n          val oldKey = S3A_PREFIX + keyWithoutPrefix\r\n          val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix\r\n          val value = confPair._2\r\n          (newKey, value)\r\n        })\r\n  }\r\n\r\n{code}\r\n\r\nThese code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part."}}}
{"metadata": {"id": "13632163", "key": "SPARK-53979", "title": "Drop temporary functions in Pandas UDF tests", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-22T03:38:12.000+0000", "updated": "2025-10-22T07:00:46.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52690\n[https://github.com/apache/spark/pull/52690]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53979?", "answer": "No description available."}}}
{"metadata": {"id": "13632157", "key": "SPARK-53978", "title": "Support logging in driver-side workers", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2025-10-21T23:41:58.000+0000", "updated": "2025-10-21T23:41:58.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53978?", "answer": "No description available."}}}
{"metadata": {"id": "13632156", "key": "SPARK-53977", "title": "Support logging in Pandas/Arrow UDTFs", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2025-10-21T23:41:32.000+0000", "updated": "2025-10-21T23:41:32.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53977?", "answer": "No description available."}}}
{"metadata": {"id": "13632155", "key": "SPARK-53976", "title": "Support logging in Pandas/Arrow UDFs", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2025-10-21T23:40:03.000+0000", "updated": "2025-10-21T23:40:03.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53976?", "answer": "No description available."}}}
{"metadata": {"id": "13632154", "key": "SPARK-53975", "title": "Add basic logging support", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-21T23:35:01.000+0000", "updated": "2025-10-24T18:47:47.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53975?", "answer": "No description available."}}}
{"metadata": {"id": "13632140", "key": "SPARK-53974", "title": "Bump Jackson 2.20.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-21T18:03:00.000+0000", "updated": "2025-10-22T09:46:42.000+0000"}, "content": {"description": "", "comments": ["Issue resolved in https://github.com/apache/spark/pull/52687"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53974?", "answer": "No description available."}}}
{"metadata": {"id": "13632133", "key": "SPARK-53973", "title": "Classify errors for AvroOptions boolean casting failure", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Siying Dong", "assignee": "Siying Dong", "labels": ["pull-request-available"], "created": "2025-10-21T17:00:06.000+0000", "updated": "2025-10-22T01:08:10.000+0000"}, "content": {"description": "IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error.", "comments": ["Issue resolved by pull request 52686\n[https://github.com/apache/spark/pull/52686]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53973?", "answer": "IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error."}}}
{"metadata": {"id": "13632131", "key": "SPARK-53972", "title": "Streaming Query RecentProgress performance regression in Classic Pyspark", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Zifei Feng", "assignee": "Zifei Feng", "labels": ["pull-request-available"], "created": "2025-10-21T16:27:41.000+0000", "updated": "2025-10-23T20:12:09.000+0000"}, "content": {"description": "We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark.\r\n\r\nWith the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64]\r\n\r\n```\r\n%python\r\nfrom datetime import datetime\r\nimport time\r\n\r\ndf = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\r\nq = df.writeStream.format(\"noop\").start()\r\nprint(\"begin waiting for progress\")\r\n\r\nprogress_list = []\r\ntime_diff_list = []\r\n\r\nnumProgress = len(q.recentProgress)\r\nwhile numProgress < 70 and q.exception() is None:\r\ntime.sleep(1)\r\nbeforeTime = datetime.now()\r\nprint(beforeTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": before we got those progress: \"+str(numProgress))\r\nrep = q.recentProgress\r\nnumProgress = len(rep)\r\nafterTime = datetime.now()\r\nprint(afterTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": after we got those progress: \"+str(numProgress))\r\ntime_diff = (afterTime - beforeTime).total_seconds()\r\nprint(\"Total Time: \"+str(time_diff) +\" seconds\")\r\nprogress_list.append(numProgress)\r\ntime_diff_list.append(time_diff)\r\n\r\nq.stop()\r\nq.awaitTermination()\r\nassert(q.exception() is None)\r\n\r\nimport pandas as pd\r\n\r\nplot_df = pd.DataFrame(\\{'numProgress': progress_list, 'time_diff': time_diff_list})\r\ndisplay(spark.createDataFrame(plot_df).orderBy(\"numProgress\").toPandas().plot.line(x=\"numProgress\", y=\"time_diff\"))\r\n```\r\nSee attachment for the generated graph. Attachment 1 is regression shown in current version. Attachment 2 is regression shown in previous version", "comments": ["Issue resolved by pull request 52688\n[https://github.com/apache/spark/pull/52688]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53972?", "answer": "We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark.\r\n\r\nWith the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64]\r\n\r\n```\r\n%python\r\nfrom datetime import datetime\r\nimport time\r\n\r\ndf = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\r\nq = df.writeStream.format(\"noop\").start()\r\nprint(\"begin waiting for progress\")\r\n\r\nprogress_list = []\r\ntime_diff_list = []\r\n\r\nnumProgress = len(q.recentProgress)\r\nwhile numProgress < 70 and q.exception() is None:\r\ntime.sleep(1)\r\nbeforeTime = datetime.now()\r\nprint(beforeTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": before we got those progress: \"+str(numProgress))\r\nrep = q.recentProgress\r\nnumProgress = len(rep)\r\nafterTime = datetime.now()\r\nprint(afterTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": after we got those progress: \"+str(numProgress))\r\ntime_diff = (afterTime - beforeTime).total_seconds()\r\nprint(\"Total Time: \"+str(time_diff) +\" seconds\")\r\nprogress_list.append(numProgress)\r\ntime_diff_list.append(time_diff)\r\n\r\nq.stop()\r\nq.awaitTermination()\r\nassert(q.exception() is None)\r\n\r\nimport pandas as pd\r\n\r\nplot_df = pd.DataFrame(\\{'numProgress': progress_list, 'time_diff': time_diff_list})\r\ndisplay(spark.createDataFrame(plot_df).orderBy(\"numProgress\").toPandas().plot.line(x=\"numProgress\", y=\"time_diff\"))\r\n```\r\nSee attachment for the generated graph. Attachment 1 is regression shown in current version. Attachment 2 is regression shown in previous version"}}}
{"metadata": {"id": "13632122", "key": "SPARK-53971", "title": "Bump zstd-jni 1.5.7-6", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-21T14:42:06.000+0000", "updated": "2025-10-21T17:36:49.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52684\n[https://github.com/apache/spark/pull/52684]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53971?", "answer": "No description available."}}}
{"metadata": {"id": "13632103", "key": "SPARK-53970", "title": "fixing a doc of from_protobuf function pyspark.", "project": "SPARK", "status": "Open", "priority": "Minor", "reporter": "mansoor", "assignee": null, "labels": [], "created": "2025-10-21T13:00:13.000+0000", "updated": "2025-10-21T13:00:13.000+0000"}, "content": {"description": "fixing a doc of from_protobuf function pyspark.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53970?", "answer": "fixing a doc of from_protobuf function pyspark."}}}
{"metadata": {"id": "13632098", "key": "SPARK-53969", "title": "Drop temporary functions in Arrow UDF tests", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-21T12:11:33.000+0000", "updated": "2025-10-21T17:38:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52682\n[https://github.com/apache/spark/pull/52682]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53969?", "answer": "No description available."}}}
{"metadata": {"id": "13632096", "key": "SPARK-53968", "title": "Store decimal precision loss conf in arithmetic expressions", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Stefan Kandic", "assignee": "Stefan Kandic", "labels": ["pull-request-available"], "created": "2025-10-21T12:08:25.000+0000", "updated": "2025-10-22T13:07:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52681\n[https://github.com/apache/spark/pull/52681]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53968?", "answer": "No description available."}}}
{"metadata": {"id": "13632086", "key": "SPARK-53967", "title": "Avoid intermediate pandas dataframe creation in df.toPandas", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-21T10:38:00.000+0000", "updated": "2025-10-23T02:15:37.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53967?", "answer": "No description available."}}}
{"metadata": {"id": "13632071", "key": "SPARK-53966", "title": "Add utility functions to detect JVM GCs", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Wan Kun", "assignee": "Wan Kun", "labels": ["pull-request-available"], "created": "2025-10-21T08:04:15.000+0000", "updated": "2025-10-24T02:19:05.000+0000"}, "content": {"description": "Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16.\r\n\r\nSo we need consider the object header when allocating spark pages.", "comments": ["Issue resolved by pull request 52678\n[https://github.com/apache/spark/pull/52678]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53966?", "answer": "Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16.\r\n\r\nSo we need consider the object header when allocating spark pages."}}}
{"metadata": {"id": "13632064", "key": "SPARK-53965", "title": " Upgrade buf plugins to v29.5", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-21T06:44:25.000+0000", "updated": "2025-10-22T02:52:55.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52677\n[https://github.com/apache/spark/pull/52677]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53965?", "answer": "No description available."}}}
{"metadata": {"id": "13632059", "key": "SPARK-53964", "title": "Simplify Java Home finding for SBT unidoc", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-21T04:22:48.000+0000", "updated": "2025-10-21T15:31:49.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52676\n[https://github.com/apache/spark/pull/52676]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53964?", "answer": "No description available."}}}
{"metadata": {"id": "13632056", "key": "SPARK-53963", "title": "Drop temporary functions in regular UDF tests", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-21T03:15:45.000+0000", "updated": "2025-10-21T15:33:28.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52674\n[https://github.com/apache/spark/pull/52674]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53963?", "answer": "No description available."}}}
{"metadata": {"id": "13632046", "key": "SPARK-53962", "title": "Upgrade ASM to 9.9", "project": "SPARK", "status": "In Progress", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-20T21:29:00.000+0000", "updated": "2025-10-20T21:39:37.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53962?", "answer": "No description available."}}}
{"metadata": {"id": "13632045", "key": "SPARK-53961", "title": "Fix `FileStreamSinkSuite` flakiness by using `walkFileTree` instead of `walk`", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-20T21:04:29.000+0000", "updated": "2025-10-21T03:36:02.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52671\n[https://github.com/apache/spark/pull/52671]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53961?", "answer": "No description available."}}}
{"metadata": {"id": "13632044", "key": "SPARK-53960", "title": "Let approx_top_k_accumulate/combine/estimate handle NULLs", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": "Yuchuan Huang", "labels": ["pull-request-available"], "created": "2025-10-20T20:57:49.000+0000", "updated": "2025-10-21T15:49:44.000+0000"}, "content": {"description": "As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs.", "comments": ["Issue resolved by pull request 52673\n[https://github.com/apache/spark/pull/52673]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53960?", "answer": "As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs."}}}
{"metadata": {"id": "13632042", "key": "SPARK-53959", "title": "Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Alex Khakhlyuk", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-20T19:52:31.000+0000", "updated": "2025-10-21T08:05:25.000+0000"}, "content": {"description": "Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state and the error is not actionable enough for the user.\r\nThis should be fixed.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53959?", "answer": "Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state and the error is not actionable enough for the user.\r\nThis should be fixed."}}}
{"metadata": {"id": "13632010", "key": "SPARK-53958", "title": "Simplify Jackson deps management by using BOM", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-20T14:19:05.000+0000", "updated": "2025-10-21T15:32:36.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52668\n[https://github.com/apache/spark/pull/52668]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53958?", "answer": "No description available."}}}
{"metadata": {"id": "13631998", "key": "SPARK-53957", "title": "Support Geography and Geometry in SRS mappings", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-20T12:48:29.000+0000", "updated": "2025-10-24T23:31:00.000+0000"}, "content": {"description": "", "comments": ["And what about this, [~uros-db]?\r\n\r\nI changed my mind - if you have something that I could take, you can ping me ;)", "Sorry, also in progress: https://github.com/apache/spark/pull/52667.", "But don't worry - there will be others soon, I can ping you then [~dekrate]!"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53957?", "answer": "No description available."}}}
{"metadata": {"id": "13631991", "key": "SPARK-53956", "title": "Support TIME in the try_make_timestamp function in Python", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-20T11:47:17.000+0000", "updated": "2025-10-23T00:19:46.000+0000"}, "content": {"description": "", "comments": ["Hi, can I take this?\r\n\r\nBut as far as I can see, there is a few try_make_timestamp functions. Which one should I modify?", "Hi [~dekrate], thank you for your help! I already have this one in progress: [https://github.com/apache/spark/pull/52666.]\r\n\r\nBut I can ping you when another task comes up - does that sound good to you?", "Ahh, I did not notice that :D\r\n\r\nI will try to find another tasks, thanks :)", "Issue resolved by pull request 52666\n[https://github.com/apache/spark/pull/52666]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53956?", "answer": "No description available."}}}
{"metadata": {"id": "13631981", "key": "SPARK-53955", "title": "Prefer to detect Java Home from env JAVA_HOME on finding jmap", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-20T10:01:06.000+0000", "updated": "2025-10-24T02:15:21.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53955?", "answer": "No description available."}}}
{"metadata": {"id": "13631978", "key": "SPARK-53954", "title": "Bump Avro 1.12.1", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-20T09:15:45.000+0000", "updated": "2025-10-21T20:24:03.000+0000"}, "content": {"description": "", "comments": ["This is resolved via [https://github.com/apache/spark/pull/52664]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53954?", "answer": "No description available."}}}
{"metadata": {"id": "13631977", "key": "SPARK-53953", "title": "Bump Avro 1.11.5", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-20T09:11:06.000+0000", "updated": "2025-10-20T17:37:31.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52663\n[https://github.com/apache/spark/pull/52663]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53953?", "answer": "No description available."}}}
{"metadata": {"id": "13631967", "key": "SPARK-53952", "title": "Balance GHA CI jobs", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-20T07:22:42.000+0000", "updated": "2025-10-21T20:24:51.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53952?", "answer": "No description available."}}}
{"metadata": {"id": "13631960", "key": "SPARK-53951", "title": "Upgrade `protobuf-java` to 4.33.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-20T06:11:00.000+0000", "updated": "2025-10-21T06:37:40.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52660\n[https://github.com/apache/spark/pull/52660]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53951?", "answer": "No description available."}}}
{"metadata": {"id": "13631959", "key": "SPARK-53950", "title": "Upgrade scala-xml to 2.4.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-20T06:06:31.000+0000", "updated": "2025-10-21T06:38:18.000+0000"}, "content": {"description": "", "comments": ["Issue resolved in https://github.com/apache/spark/pull/52659"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53950?", "answer": "No description available."}}}
{"metadata": {"id": "13631958", "key": "SPARK-53949", "title": "Use `Utils. getRootCause` instead of `Throwables.getRootCause`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-20T06:00:08.000+0000", "updated": "2025-10-21T06:38:04.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52658\n[https://github.com/apache/spark/pull/52658]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53949?", "answer": "No description available."}}}
{"metadata": {"id": "13631941", "key": "SPARK-53948", "title": "Fix deadlock in Observation", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "L. C. Hsieh", "assignee": "L. C. Hsieh", "labels": ["pull-request-available"], "created": "2025-10-19T23:22:23.000+0000", "updated": "2025-10-20T16:57:25.000+0000"}, "content": {"description": "Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval.\r\n\r\nBut it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics.\r\n\r\nThis locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don’t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch.", "comments": ["Issue resolved by pull request 52657\n[https://github.com/apache/spark/pull/52657]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53948?", "answer": "Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval.\r\n\r\nBut it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics.\r\n\r\nThis locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don’t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch."}}}
{"metadata": {"id": "13631898", "key": "SPARK-53947", "title": "Let approx_top_k handle NULLs", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": "Yuchuan Huang", "labels": ["pull-request-available"], "created": "2025-10-18T21:55:43.000+0000", "updated": "2025-10-20T22:42:49.000+0000"}, "content": {"description": "Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation. ", "comments": ["Issue resolved by pull request 52655\n[https://github.com/apache/spark/pull/52655]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53947?", "answer": "Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation. "}}}
{"metadata": {"id": "13631864", "key": "SPARK-53946", "title": "Upgrade SBT to 1.11.7", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "labels": ["pull-request-available"], "created": "2025-10-17T23:44:12.000+0000", "updated": "2025-10-21T23:41:46.000+0000"}, "content": {"description": "We last upgraded SBT two years ago. Let's upgrade SBT to the latest version.", "comments": ["Issue resolved by pull request 52653\n[https://github.com/apache/spark/pull/52653]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53946?", "answer": "We last upgraded SBT two years ago. Let's upgrade SBT to the latest version."}}}
{"metadata": {"id": "13631863", "key": "SPARK-53945", "title": "Upgrade semanticdb-shared to 4.13.10", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "labels": ["pull-request-available"], "created": "2025-10-17T23:20:33.000+0000", "updated": "2025-10-20T06:49:07.000+0000"}, "content": {"description": "Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly.\r\n\r\nhttps://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3", "comments": ["Issue resolved in https://github.com/apache/spark/pull/52652"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53945?", "answer": "Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly.\r\n\r\nhttps://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3"}}}
{"metadata": {"id": "13631851", "key": "SPARK-53944", "title": "Support `spark.kubernetes.executor.useDriverPodIP`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-17T18:50:11.000+0000", "updated": "2025-10-24T15:18:36.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52650\n[https://github.com/apache/spark/pull/52650]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53944?", "answer": "No description available."}}}
{"metadata": {"id": "13631807", "key": "SPARK-53943", "title": "Add examples for function unwrap_udt", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-17T08:35:09.000+0000", "updated": "2025-10-17T10:04:36.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52647\n[https://github.com/apache/spark/pull/52647]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53943?", "answer": "No description available."}}}
{"metadata": {"id": "13631790", "key": "SPARK-53942", "title": "Support changing stateless shuffle partitions upon restart of streaming query", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "labels": ["pull-request-available"], "created": "2025-10-17T07:00:53.000+0000", "updated": "2025-10-25T08:57:53.000+0000"}, "content": {"description": "We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this.\r\n\r\nThe main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.", "comments": ["Issue resolved by pull request 52645\n[https://github.com/apache/spark/pull/52645]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53942?", "answer": "We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this.\r\n\r\nThe main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc."}}}
{"metadata": {"id": "13631787", "key": "SPARK-53941", "title": "Support AQE in stateless streaming workloads", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "labels": ["pull-request-available"], "created": "2025-10-17T06:50:11.000+0000", "updated": "2025-10-26T14:14:20.000+0000"}, "content": {"description": "We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.", "comments": ["PR is up for review. Will rename the PR.", "Issue resolved by pull request 52642\n[https://github.com/apache/spark/pull/52642]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53941?", "answer": "We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads."}}}
{"metadata": {"id": "13631773", "key": "SPARK-53940", "title": "Function version() should return full spark version instead of short version", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-17T03:37:37.000+0000", "updated": "2025-10-21T15:58:47.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53940?", "answer": "No description available."}}}
{"metadata": {"id": "13631761", "key": "SPARK-53939", "title": "Use batch.num_columns instead of len(batch.columns).", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": ["pull-request-available"], "created": "2025-10-16T21:02:40.000+0000", "updated": "2025-10-17T04:12:42.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52639\n[https://github.com/apache/spark/pull/52639]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53939?", "answer": "No description available."}}}
{"metadata": {"id": "13631731", "key": "SPARK-53938", "title": "Fix decimal rescaling in LocalDataToArrowConversion", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-16T13:14:27.000+0000", "updated": "2025-10-20T06:37:13.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52637\n[https://github.com/apache/spark/pull/52637]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53938?", "answer": "No description available."}}}
{"metadata": {"id": "13631714", "key": "SPARK-53937", "title": "SparkSQL partition overwrite is not an atomic operation.", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Chenyu Zheng", "assignee": null, "labels": [], "created": "2025-10-16T10:00:59.000+0000", "updated": "2025-10-16T10:01:37.000+0000"}, "content": {"description": "I found that SparkSQL partition overwrite is not an atomic operation. When SparkSQL application rewrites existing table partition, [delete the matching partition]([https://github.com/apache/spark/blob/24a6abf34d253162055c8b9bd0030bf9a2ca75b1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L136]), then run the job.\r\n\r\nDuring task execution, the Hive partition is not deleted, but the data in the filesystem is deleted. If you read data while the job is running, you will read empty data. If the job is interrupted, the data will be lost.\r\n\r\nNote: Only for spark.sql.hive.convertMetastoreOrc or spark.sql.hive.convertMetastoreParquet is true. It is not problem for hive serde.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53937?", "answer": "I found that SparkSQL partition overwrite is not an atomic operation. When SparkSQL application rewrites existing table partition, [delete the matching partition]([https://github.com/apache/spark/blob/24a6abf34d253162055c8b9bd0030bf9a2ca75b1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L136]), then run the job.\r\n\r\nDuring task execution, the Hive partition is not deleted, but the data in the filesystem is deleted. If you read data while the job is running, you will read empty data. If the job is interrupted, the data will be lost.\r\n\r\nNote: Only for spark.sql.hive.convertMetastoreOrc or spark.sql.hive.convertMetastoreParquet is true. It is not problem for hive serde."}}}
{"metadata": {"id": "13631695", "key": "SPARK-53936", "title": "Upgrade sbt-pom-reader from 2.4.0 to 2.5.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yuming Wang", "assignee": "Denis Pyshev", "labels": ["pull-request-available"], "created": "2025-10-16T08:23:08.000+0000", "updated": "2025-10-16T15:01:23.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52580\n[https://github.com/apache/spark/pull/52580]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53936?", "answer": "No description available."}}}
{"metadata": {"id": "13631684", "key": "SPARK-53935", "title": "SBT assembly should correct handle META-INF", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-16T06:06:57.000+0000", "updated": "2025-10-16T09:37:22.000+0000"}, "content": {"description": "", "comments": ["Issue resolved in https://github.com/apache/spark/pull/52636"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53935?", "answer": "No description available."}}}
{"metadata": {"id": "13631683", "key": "SPARK-53934", "title": "Initial implement Connect JDBC driver", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-16T05:53:06.000+0000", "updated": "2025-10-24T02:31:11.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52705\n[https://github.com/apache/spark/pull/52705]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53934?", "answer": "No description available."}}}
{"metadata": {"id": "13631680", "key": "SPARK-53933", "title": "Add `Apache Iceberg` example", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-16T03:56:24.000+0000", "updated": "2025-10-16T15:27:18.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 394\n[https://github.com/apache/spark-kubernetes-operator/pull/394]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53933?", "answer": "No description available."}}}
{"metadata": {"id": "13631679", "key": "SPARK-53932", "title": "Add `FAIR` schedule examples with Spark (Connect|Thrift) Servers", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-16T02:46:58.000+0000", "updated": "2025-10-16T06:27:12.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 393\n[https://github.com/apache/spark-kubernetes-operator/pull/393]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53932?", "answer": "No description available."}}}
{"metadata": {"id": "13631678", "key": "SPARK-53931", "title": "Fix scheduled job for numpy 2.1.3", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-16T02:19:30.000+0000", "updated": "2025-10-16T19:52:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52633\n[https://github.com/apache/spark/pull/52633]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53931?", "answer": "No description available."}}}
{"metadata": {"id": "13631673", "key": "SPARK-53930", "title": "Support TIME in the make_timestamp function in Python", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-16T00:09:18.000+0000", "updated": "2025-10-23T00:23:35.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52648.", "Issue resolved by pull request 52648\n[https://github.com/apache/spark/pull/52648]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53930?", "answer": "No description available."}}}
{"metadata": {"id": "13631672", "key": "SPARK-53929", "title": "Support TIME in the make_timestamp and try_make_timestamp functions in Scala", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-16T00:09:12.000+0000", "updated": "2025-10-16T05:52:43.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52631.", "Issue resolved by pull request 52631\n[https://github.com/apache/spark/pull/52631]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53929?", "answer": "No description available."}}}
{"metadata": {"id": "13631670", "key": "SPARK-53928", "title": "Enhance DSV2 partition filtering using catalyst expression", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Szehon Ho", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-15T22:35:20.000+0000", "updated": "2025-10-16T13:59:45.000+0000"}, "content": {"description": "Currently, Spark converts Catalyst Expression to either Filter or Predicate and pushes it to DSV2 via SupportsPushdownFilters and SupportsPushdownV2Filters API's.\r\n\r\nHowever, some Spark filters may not convert cleanly.  For example, trim(part_col) = 'a'.  There are cases where DSV2 can return the exact partition value(s) to spark for its InputPartition, and Spark can use the original catalyst expression for filtering.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53928?", "answer": "Currently, Spark converts Catalyst Expression to either Filter or Predicate and pushes it to DSV2 via SupportsPushdownFilters and SupportsPushdownV2Filters API's.\r\n\r\nHowever, some Spark filters may not convert cleanly.  For example, trim(part_col) = 'a'.  There are cases where DSV2 can return the exact partition value(s) to spark for its InputPartition, and Spark can use the original catalyst expression for filtering."}}}
{"metadata": {"id": "13631661", "key": "SPARK-53927", "title": "Kinesis tests are broken", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Vlad Rozov", "assignee": "Vlad Rozov", "labels": ["pull-request-available"], "created": "2025-10-15T21:04:05.000+0000", "updated": "2025-10-24T15:50:02.000+0000"}, "content": {"description": "Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n[info]   at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n[info]   at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[error] Uncaught exception when running org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite: java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n{code}\r\n\r\n", "comments": ["Using maven leads to the same error.", "Issue resolved in https://github.com/apache/spark/pull/52630"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53927?", "answer": "Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n[info]   at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n[info]   at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[error] Uncaught exception when running org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite: java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n{code}\r\n\r\n"}}}
{"metadata": {"id": "13631660", "key": "SPARK-53926", "title": "Document newly added `core` module configurations", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T20:45:58.000+0000", "updated": "2025-10-16T02:02:44.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52626\n[https://github.com/apache/spark/pull/52626]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53926?", "answer": "No description available."}}}
{"metadata": {"id": "13631657", "key": "SPARK-53925", "title": "Use `MacOS 26` in `build_maven_java21_macos15.yml`", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T19:30:49.000+0000", "updated": "2025-10-16T05:40:10.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52625\n[https://github.com/apache/spark/pull/52625]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53925?", "answer": "No description available."}}}
{"metadata": {"id": "13631656", "key": "SPARK-53924", "title": "Reload DSv2 tables in views created using plans on each access", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "labels": [], "created": "2025-10-15T19:27:17.000+0000", "updated": "2025-10-15T22:30:20.000+0000"}, "content": {"description": "The current problem is that the view definition in the session catalog captures the analyzed plan that references DataSourceV2Relation and Table. If a connector doesn’t have an internal cache and produces a new Table object after TableCatalog$load, Table referenced in the view will become orphan and there will be no way to refresh it unless that Table instance auto refreshes on each scan (super dangerous).", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53924?", "answer": "The current problem is that the view definition in the session catalog captures the analyzed plan that references DataSourceV2Relation and Table. If a connector doesn’t have an internal cache and produces a new Table object after TableCatalog$load, Table referenced in the view will become orphan and there will be no way to refresh it unless that Table instance auto refreshes on each scan (super dangerous)."}}}
{"metadata": {"id": "13631654", "key": "SPARK-53923", "title": "Rename `spark.executor.(log -> logs).redirectConsoleOutputs`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T17:41:15.000+0000", "updated": "2025-10-15T18:58:53.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52624\n[https://github.com/apache/spark/pull/52624]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53923?", "answer": "No description available."}}}
{"metadata": {"id": "13631651", "key": "SPARK-53922", "title": "Introduce Geography and Geometry physical types", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-15T17:17:28.000+0000", "updated": "2025-10-23T08:00:08.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52629.", "Issue resolved by pull request 52629\n[https://github.com/apache/spark/pull/52629]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53922?", "answer": "No description available."}}}
{"metadata": {"id": "13631650", "key": "SPARK-53921", "title": "Introduce Geography and Geometry data types to PySpark API", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-15T17:16:54.000+0000", "updated": "2025-10-22T00:15:03.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52627.", "Issue resolved by pull request 52627\n[https://github.com/apache/spark/pull/52627]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53921?", "answer": "No description available."}}}
{"metadata": {"id": "13631649", "key": "SPARK-53920", "title": "Introduce Geography and Geometry data types to Java API", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": "Uroš Bojanić", "labels": ["pull-request-available"], "created": "2025-10-15T17:16:46.000+0000", "updated": "2025-10-23T18:06:49.000+0000"}, "content": {"description": "", "comments": ["Work in progress: https://github.com/apache/spark/pull/52623.", "Issue resolved by pull request 52623\n[https://github.com/apache/spark/pull/52623]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53920?", "answer": "No description available."}}}
{"metadata": {"id": "13631642", "key": "SPARK-53919", "title": "Make Maven plugins up-to-date", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T15:49:32.000+0000", "updated": "2025-10-15T22:17:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52622\n[https://github.com/apache/spark/pull/52622]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53919?", "answer": "No description available."}}}
{"metadata": {"id": "13631612", "key": "SPARK-53918", "title": "asyncCheckpoint.enabled incompatible with transformWithStateInPandas (StatefulProcessor)", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Elvinas Pilypas", "assignee": null, "labels": [], "created": "2025-10-15T11:51:51.000+0000", "updated": "2025-10-16T13:48:03.000+0000"}, "content": {"description": "In structured streaming job, we use transformWithStateInPandas stateful processing and we have com.databricks.sql.streaming.state.RocksDBStateStoreProvider enabled. Everything works as expected.\r\n\r\nBut when we enable:\r\nspark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"true\")\r\n\r\nWe get an error: org.apache.spark.SparkUnsupportedOperationException: Synchronous commit is not supported. Use asynchronous commit.\r\n \r\nWe have successfully replicated that in notebook, and also tried the same thing with applyInPandasWithState.\r\n * transformWithStateInPandas - throws Synchronous commit error\r\n * applyInPandasWithState - works as expected\r\n\r\nWe followed this guide: [https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/async-checkpointing] and also used chatGPT for references.\r\n\r\nLooking through the stacktrace, It seems like the issue is where {{TransformWithStateInPandasExec}} uses the sync {{commit()}} instead of the async {{{}commitAsync(){}}}.\r\n\r\n \r\n\r\nPlease check the attached notebook below", "comments": ["Looks like your issue is mostly scoped to Databricks. Could you please contact the support in Databricks? This JIRA is about \"Apache\" Spark."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53918?", "answer": "In structured streaming job, we use transformWithStateInPandas stateful processing and we have com.databricks.sql.streaming.state.RocksDBStateStoreProvider enabled. Everything works as expected.\r\n\r\nBut when we enable:\r\nspark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"true\")\r\n\r\nWe get an error: org.apache.spark.SparkUnsupportedOperationException: Synchronous commit is not supported. Use asynchronous commit.\r\n \r\nWe have successfully replicated that in notebook, and also tried the same thing with applyInPandasWithState.\r\n * transformWithStateInPandas - throws Synchronous commit error\r\n * applyInPandasWithState - works as expected\r\n\r\nWe followed this guide: [https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/async-checkpointing] and also used chatGPT for references.\r\n\r\nLooking through the stacktrace, It seems like the issue is where {{TransformWithStateInPandasExec}} uses the sync {{commit()}} instead of the async {{{}commitAsync(){}}}.\r\n\r\n \r\n\r\nPlease check the attached notebook below"}}}
{"metadata": {"id": "13631611", "key": "SPARK-53917", "title": "[CONNECT] Supporting large LocalRelations", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Alex Khakhlyuk", "assignee": "Alex Khakhlyuk", "labels": ["pull-request-available"], "created": "2025-10-15T11:48:05.000+0000", "updated": "2025-10-22T13:49:49.000+0000"}, "content": {"description": "h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can’t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are sent directly as part of the ExecutePlanRequest.\r\n\r\n!image-2025-10-15-13-50-04-179.png!\r\n\r\nLarger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it.\r\n\r\n!image-2025-10-15-13-50-44-333.png!\r\n\r\n \r\n\r\nThe server reads the data from the BlockManager as a stream and tries to create proto.LocalRelation via\r\n{quote}proto.Relation\r\n.newBuilder()\r\n.getLocalRelation\r\n.getParserForType\r\n.parseFrom(blockData.toInputStream())\r\n{quote}\r\nThis fails, because java protobuf library has a 2GB limit on deserializing protobuf messages from a string.\r\n{quote}org.sparkproject.connect.com.google.protobuf.InvalidProtocolBufferException) CodedInputStream encountered an embedded string or message which claimed to have negative size.\r\n{quote}\r\n!image-2025-10-15-13-53-40-306.png!\r\n\r\nTo fix this, I propose avoiding the protobuf layer during the serialization on the client and deserialization on the server. Instead of caching the full protobuf LocalRelation message, we cache the data and schema as separate artifacts, send two hashes \\{data_hash, schema_hash} to the server, load them both from BlockManager directly and create a LocalRelation on the server based on the unpacked data and schema.\r\n\r\n!image-2025-10-15-13-56-46-840.png!\r\n\r\nAfter creating a prototype with the new proto message, I discovered that there are additional limits for CachedLocalRelations. Both the Scala Client and the Server store the data in a single Java {{{}Array[Byte]{}}}, which has a 2GB size limit in Java. To avoid this limit, I propose transferring data in chunks. The Python and Scala clients will split data into multiple Arrow batches and upload them separately to the server. Each batch will be uploaded and stored a separate artifact. The Server will then load and process each batch separately. We will keep batch sizes around 16MB (TBD), well below the 2GB limit. This way we will avoid 2GB limits on both clients and on the server.\r\n\r\n!image-2025-10-15-13-59-08-081.png!\r\n\r\nThe final proto message looks like this:\r\n{quote}message ChunkedCachedLocalRelation {\r\n  // (Required) A list of sha-256 hashes for representing LocalRelation.data.\r\n  repeated string dataHashes = 1;\r\n  // (Optional) A sha-256 hash of the serialized LocalRelation.schema.\r\n  optional string schemaHash = 2;\r\n}\r\n{quote}\r\nImplementation details are discussed in the PR [https://github.com/apache/spark/pull/52613].", "comments": ["Issue resolved by pull request 52613\n[https://github.com/apache/spark/pull/52613]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53917?", "answer": "h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can’t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are sent directly as part of the ExecutePlanRequest.\r\n\r\n!image-2025-10-15-13-50-04-179.png!\r\n\r\nLarger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it.\r\n\r\n!image-2025-10-15-13-50-44-333.png!\r\n\r\n \r\n\r\nThe server reads the data from the BlockManager as a stream and tries to create proto.LocalRelation via\r\n{quote}proto.Relation\r\n.newBuilder()\r\n.getLocalRelation\r\n.getParserForType\r\n.parseFrom(blockData.toInputStream())\r\n{quote}\r\nThis fails, because java protobuf library has a 2GB limit on deserializing protobuf messages from a string.\r\n{quote}org.sparkproject.connect.com.google.protobuf.InvalidProtocolBufferException) CodedInputStream encountered an embedded string or message which claimed to have negative size.\r\n{quote}\r\n!image-2025-10-15-13-53-40-306.png!\r\n\r\nTo fix this, I propose avoiding the protobuf layer during the serialization on the client and deserialization on the server. Instead of caching the full protobuf LocalRelation message, we cache the data and schema as separate artifacts, send two hashes \\{data_hash, schema_hash} to the server, load them both from BlockManager directly and create a LocalRelation on the server based on the unpacked data and schema.\r\n\r\n!image-2025-10-15-13-56-46-840.png!\r\n\r\nAfter creating a prototype with the new proto message, I discovered that there are additional limits for CachedLocalRelations. Both the Scala Client and the Server store the data in a single Java {{{}Array[Byte]{}}}, which has a 2GB size limit in Java. To avoid this limit, I propose transferring data in chunks. The Python and Scala clients will split data into multiple Arrow batches and upload them separately to the server. Each batch will be uploaded and stored a separate artifact. The Server will then load and process each batch separately. We will keep batch sizes around 16MB (TBD), well below the 2GB limit. This way we will avoid 2GB limits on both clients and on the server.\r\n\r\n!image-2025-10-15-13-59-08-081.png!\r\n\r\nThe final proto message looks like this:\r\n{quote}message ChunkedCachedLocalRelation {\r\n  // (Required) A list of sha-256 hashes for representing LocalRelation.data.\r\n  repeated string dataHashes = 1;\r\n  // (Optional) A sha-256 hash of the serialized LocalRelation.schema.\r\n  optional string schemaHash = 2;\r\n}\r\n{quote}\r\nImplementation details are discussed in the PR [https://github.com/apache/spark/pull/52613]."}}}
{"metadata": {"id": "13631606", "key": "SPARK-53916", "title": "Deduplicate the variables in PythonArrowInput", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-15T11:08:57.000+0000", "updated": "2025-10-16T02:57:15.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52621\n[https://github.com/apache/spark/pull/52621]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53916?", "answer": "No description available."}}}
{"metadata": {"id": "13631583", "key": "SPARK-53915", "title": "Add RealTimeScanExec and ability to execute long running batches", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-15T05:59:45.000+0000", "updated": "2025-10-24T23:06:32.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53915?", "answer": "No description available."}}}
{"metadata": {"id": "13631579", "key": "SPARK-53914", "title": "Add Connect JDBC module", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-15T04:52:40.000+0000", "updated": "2025-10-24T02:33:05.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52619\n[https://github.com/apache/spark/pull/52619]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53914?", "answer": "No description available."}}}
{"metadata": {"id": "13631578", "key": "SPARK-53913", "title": "Document newly added K8s configurations", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T03:33:44.000+0000", "updated": "2025-10-24T15:18:17.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52618\n[https://github.com/apache/spark/pull/52618]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53913?", "answer": "No description available."}}}
{"metadata": {"id": "13631575", "key": "SPARK-53912", "title": "Add `cluster-preview.yaml` and `spark-connect-server-preview.yaml`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T02:51:27.000+0000", "updated": "2025-10-15T15:01:19.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 392\n[https://github.com/apache/spark-kubernetes-operator/pull/392]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53912?", "answer": "No description available."}}}
{"metadata": {"id": "13631573", "key": "SPARK-53911", "title": "Support `SPARK_VERSION` placeholder in container image names", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T01:43:03.000+0000", "updated": "2025-10-15T02:44:02.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 391\n[https://github.com/apache/spark-kubernetes-operator/pull/391]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53911?", "answer": "No description available."}}}
{"metadata": {"id": "13631571", "key": "SPARK-53910", "title": "Add `StatefulSet`-based SparkApp example", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-15T00:25:20.000+0000", "updated": "2025-10-15T00:46:55.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 390\n[https://github.com/apache/spark-kubernetes-operator/pull/390]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53910?", "answer": "No description available."}}}
{"metadata": {"id": "13631569", "key": "SPARK-53909", "title": "Fix RBAC to allow `Spark` driver to create `StatefulSet`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T23:36:37.000+0000", "updated": "2025-10-15T00:46:08.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 389\n[https://github.com/apache/spark-kubernetes-operator/pull/389]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53909?", "answer": "No description available."}}}
{"metadata": {"id": "13631564", "key": "SPARK-53908", "title": "Fix observations on Spark Connect with plan cache", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": ["pull-request-available"], "created": "2025-10-14T22:31:34.000+0000", "updated": "2025-10-16T20:06:42.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52616\r\nhttps://github.com/apache/spark/pull/52616"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53908?", "answer": "No description available."}}}
{"metadata": {"id": "13631556", "key": "SPARK-53907", "title": "Support `spark.kubernetes.allocation.maximum`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T20:52:12.000+0000", "updated": "2025-10-24T15:18:05.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52615\n[https://github.com/apache/spark/pull/52615]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53907?", "answer": "No description available."}}}
{"metadata": {"id": "13631553", "key": "SPARK-53906", "title": "Protect `ExecutorPodsAllocator.numOutstandingPods` as `protected val`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T19:04:03.000+0000", "updated": "2025-10-24T15:17:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52614\n[https://github.com/apache/spark/pull/52614]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53906?", "answer": "No description available."}}}
{"metadata": {"id": "13631546", "key": "SPARK-53905", "title": "Refactor RelationResolution to enable code reuse", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "labels": [], "created": "2025-10-14T18:04:01.000+0000", "updated": "2025-10-14T18:04:01.000+0000"}, "content": {"description": "Refactor RelationResolution to enable code reuse", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53905?", "answer": "Refactor RelationResolution to enable code reuse"}}}
{"metadata": {"id": "13631535", "key": "SPARK-53904", "title": "Use `Consumer.poll(Duration)` instead of `Consumer.poll(long)`", "project": "SPARK", "status": "In Progress", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-14T16:18:02.000+0000", "updated": "2025-10-14T20:54:32.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53904?", "answer": "No description available."}}}
{"metadata": {"id": "13631526", "key": "SPARK-53903", "title": "Performance degradation for PySpark apis at scale as compared to Scala apis", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Sumit Kumar", "assignee": null, "labels": [], "created": "2025-10-14T14:58:03.000+0000", "updated": "2025-10-14T14:58:13.000+0000"}, "content": {"description": "Customers love PySpark and the flexibility of using several python libraries as part of our workflows. I've a unique scenario where this specific usecase has multiple tables with around 10k columns and some of those columns have array datatype that when exploded, contain ~1k columns each.\r\n\r\n*Issues that we are facing:*\r\n * Frequent driver OOM depending on the use case and how many columns are involved in the logic and how many array type columns are exploded. There is frequent GC, slowing down the workflows.\r\n * We tried equivalent scala apis and the performance as well latency seemed a lot better (no OOM and significantly less GC overheads).\r\n\r\n*Here is what we understand so far from thread and memory dumps:*\r\n * driver ends up having open references for every pyspark object created in the python vm because of py4j bridge-based communication implementation for pyspark apis. and the garbage keeps accumulating on driver ultimately leading to OOM\r\n * even if we delete python references in pyspark code (for example:  del df_dummy1) and run \"gc.collect()\" specifically, we are not able to ease the memory pressure. Python gc or python triggered gc via py4j bridge in the driver doesn't seem to be that good.\r\n\r\nThis is not a typical workload but we have multiple such usecases and we are debating if it's worth changing existing workflows to scala just like that (existing DEs are more comfortable with PySpark, there is cost of migration as well that we will have to convince our management to approve)\r\n\r\n*ASK:*\r\n\r\nThis Jira includes a sample notebook that reproduces what our usecases see. We are seeking community feedback on such a usecase and if there are ideas to improve this situation further other than migrating to Scala apis. Any PySpark improvement ideas that could help? ", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53903?", "answer": "Customers love PySpark and the flexibility of using several python libraries as part of our workflows. I've a unique scenario where this specific usecase has multiple tables with around 10k columns and some of those columns have array datatype that when exploded, contain ~1k columns each.\r\n\r\n*Issues that we are facing:*\r\n * Frequent driver OOM depending on the use case and how many columns are involved in the logic and how many array type columns are exploded. There is frequent GC, slowing down the workflows.\r\n * We tried equivalent scala apis and the performance as well latency seemed a lot better (no OOM and significantly less GC overheads).\r\n\r\n*Here is what we understand so far from thread and memory dumps:*\r\n * driver ends up having open references for every pyspark object created in the python vm because of py4j bridge-based communication implementation for pyspark apis. and the garbage keeps accumulating on driver ultimately leading to OOM\r\n * even if we delete python references in pyspark code (for example:  del df_dummy1) and run \"gc.collect()\" specifically, we are not able to ease the memory pressure. Python gc or python triggered gc via py4j bridge in the driver doesn't seem to be that good.\r\n\r\nThis is not a typical workload but we have multiple such usecases and we are debating if it's worth changing existing workflows to scala just like that (existing DEs are more comfortable with PySpark, there is cost of migration as well that we will have to convince our management to approve)\r\n\r\n*ASK:*\r\n\r\nThis Jira includes a sample notebook that reproduces what our usecases see. We are seeking community feedback on such a usecase and if there are ideas to improve this situation further other than migrating to Scala apis. Any PySpark improvement ideas that could help? "}}}
{"metadata": {"id": "13631518", "key": "SPARK-53902", "title": "Add tree node pattern bits for supported expressions in ParameterizedQuery argument list", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Mihailo Aleksic", "assignee": "Mihailo Aleksic", "labels": ["pull-request-available"], "created": "2025-10-14T13:57:50.000+0000", "updated": "2025-10-15T16:30:46.000+0000"}, "content": {"description": "In this PR I propose that we add tree node pattern bits for supported expressions in ParameterizedQuery argument list to prepare implementation of parameters in single-pass framework.", "comments": ["Issue resolved by pull request 52611\n[https://github.com/apache/spark/pull/52611]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53902?", "answer": "In this PR I propose that we add tree node pattern bits for supported expressions in ParameterizedQuery argument list to prepare implementation of parameters in single-pass framework."}}}
{"metadata": {"id": "13631515", "key": "SPARK-53901", "title": "Address memory leak in SparkConnectAddArtifactsHandler", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-14T13:31:12.000+0000", "updated": "2025-10-24T05:13:24.000+0000"}, "content": {"description": "[stagedArtifacts|https://github.com/apache/spark/blob/master/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L48-L49] buffer is never cleared when artifacts are flushed/on error", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53901?", "answer": "[stagedArtifacts|https://github.com/apache/spark/blob/master/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L48-L49] buffer is never cleared when artifacts are flushed/on error"}}}
{"metadata": {"id": "13631511", "key": "SPARK-53900", "title": "Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": "Venkata Sai Akhil Gudesa", "labels": ["pull-request-available"], "created": "2025-10-14T12:42:32.000+0000", "updated": "2025-10-24T05:13:24.000+0000"}, "content": {"description": " \r\n\r\nA bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253] during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait).\r\nh2. Root Cause\r\nexecutionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS)  // ← BUG\r\n{*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed):\r\n # Math.max(1, negative_value) clamps to 1 nanosecond\r\n\r\n # Math.min(progressInterval_ns, 1) remains 1 nanosecond\r\n\r\n # Integer division: 1 / 1,000,000 = 0 milliseconds\r\n\r\n # wait(0) in Java means *wait indefinitely until notified*\r\n\r\n # No notification arrives (execution already completed), thread hangs forever\r\n\r\nWhile one the loop conditions guards against deadlineTimeNs < System.nanoTime(), it isn’t sufficient as the deadline can elapse while inside the loop (the time is freshly fetched in the latter timeout calculation). The probability of occurence can exacerbated by GC pauses\r\nh2. Conditions Required for Bug to Trigger\r\n\r\nThe bug manifests when *all* of the following conditions are met:\r\n # *Reattachable execution enabled* (CONNECT_EXECUTE_REATTACHABLE_ENABLED = true)\r\n\r\n # *Execution completes prior* to the deadline within the inner loop\r\n\r\n # (all responses sent before deadline)\r\n\r\n # *Deadline passes* within the inner loop\r\n\r\nh2. Proposed fix\r\n\r\nHave timeoutNs always contain a positive value.\r\nexecutionObserver.responseLock.wait(Math.max(1, timeoutNs / NANOS_PER_MILLIS))", "comments": ["Issue resolved by pull request 52609\n[https://github.com/apache/spark/pull/52609]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53900?", "answer": " \r\n\r\nA bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253] during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait).\r\nh2. Root Cause\r\nexecutionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS)  // ← BUG\r\n{*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed):\r\n # Math.max(1, negative_value) clamps to 1 nanosecond\r\n\r\n # Math.min(progressInterval_ns, 1) remains 1 nanosecond\r\n\r\n # Integer division: 1 / 1,000,000 = 0 milliseconds\r\n\r\n # wait(0) in Java means *wait indefinitely until notified*\r\n\r\n # No notification arrives (execution already completed), thread hangs forever\r\n\r\nWhile one the loop conditions guards against deadlineTimeNs < System.nanoTime(), it isn’t sufficient as the deadline can elapse while inside the loop (the time is freshly fetched in the latter timeout calculation). The probability of occurence can exacerbated by GC pauses\r\nh2. Conditions Required for Bug to Trigger\r\n\r\nThe bug manifests when *all* of the following conditions are met:\r\n # *Reattachable execution enabled* (CONNECT_EXECUTE_REATTACHABLE_ENABLED = true)\r\n\r\n # *Execution completes prior* to the deadline within the inner loop\r\n\r\n # (all responses sent before deadline)\r\n\r\n # *Deadline passes* within the inner loop\r\n\r\nh2. Proposed fix\r\n\r\nHave timeoutNs always contain a positive value.\r\nexecutionObserver.responseLock.wait(Math.max(1, timeoutNs / NANOS_PER_MILLIS))"}}}
{"metadata": {"id": "13631499", "key": "SPARK-53899", "title": "Update `Operator SDK` to 5.1.4", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Attila Mészáros", "assignee": "Attila Mészáros", "labels": ["pull-request-available"], "created": "2025-10-14T11:08:44.000+0000", "updated": "2025-10-14T19:51:35.000+0000"}, "content": {"description": "see JOSDK release: https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.1.4", "comments": ["Issue resolved by pull request 387\n[https://github.com/apache/spark-kubernetes-operator/pull/387]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53899?", "answer": "see JOSDK release: https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.1.4"}}}
{"metadata": {"id": "13631452", "key": "SPARK-53898", "title": "MapOutputTrackerMaster.shufflestatuses is mistakenly cleaned by Shuffle cleanup", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "wuyi", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-14T03:48:40.000+0000", "updated": "2025-10-22T13:45:19.000+0000"}, "content": {"description": "MapOutputTrackerMaster.shufflestatuses can be mistakenly cleaned by Shuffle Cleanup feature, leading to SparkException (crashing the SparkContext) by the subsequent access to that already removed shuffle metadata. A real case (limited to local cluster currently) is the ongoing subquery could access the shuffle metadata which has been already cleanedup after the main query completes. See the detailed discussion at: [https://github.com/apache/spark/pull/52213#discussion_r2415632474].", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53898?", "answer": "MapOutputTrackerMaster.shufflestatuses can be mistakenly cleaned by Shuffle Cleanup feature, leading to SparkException (crashing the SparkContext) by the subsequent access to that already removed shuffle metadata. A real case (limited to local cluster currently) is the ongoing subquery could access the shuffle metadata which has been already cleanedup after the main query completes. See the detailed discussion at: [https://github.com/apache/spark/pull/52213#discussion_r2415632474]."}}}
{"metadata": {"id": "13631448", "key": "SPARK-53897", "title": "Add dependency checks for Python-related tests in the connect module", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-14T03:07:01.000+0000", "updated": "2025-10-15T02:33:22.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52588\n[https://github.com/apache/spark/pull/52588]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53897?", "answer": "No description available."}}}
{"metadata": {"id": "13631447", "key": "SPARK-53896", "title": "Enable `spark.io.compression.lzf.parallel.enabled` by default", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T02:43:17.000+0000", "updated": "2025-10-14T06:20:08.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52603\n[https://github.com/apache/spark/pull/52603]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53896?", "answer": "No description available."}}}
{"metadata": {"id": "13631446", "key": "SPARK-53895", "title": "Add a missing test for ContinuousMemorySink", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "labels": ["pull-request-available"], "created": "2025-10-14T02:24:45.000+0000", "updated": "2025-10-15T02:36:23.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52602\n[https://github.com/apache/spark/pull/52602]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53895?", "answer": "No description available."}}}
{"metadata": {"id": "13631444", "key": "SPARK-53894", "title": "Upgrade `docker-java` to 3.6.0", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T02:22:37.000+0000", "updated": "2025-10-24T15:42:48.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52601\n[https://github.com/apache/spark/pull/52601]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53894?", "answer": "No description available."}}}
{"metadata": {"id": "13631443", "key": "SPARK-53893", "title": "Regenerate benchmark results after upgrading to Scala 2.13.17", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-14T02:09:45.000+0000", "updated": "2025-10-14T03:17:56.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52600\n[https://github.com/apache/spark/pull/52600]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53893?", "answer": "No description available."}}}
{"metadata": {"id": "13631434", "key": "SPARK-53892", "title": "Use `DescribeTopicsResult.allTopicNames` instead of the deprecated `all` API", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-13T20:59:30.000+0000", "updated": "2025-10-14T00:55:32.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52597\n[https://github.com/apache/spark/pull/52597]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53892?", "answer": "No description available."}}}
{"metadata": {"id": "13631427", "key": "SPARK-53891", "title": "Model DSV2 Commit Operation Metrics API", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Szehon Ho", "assignee": "Szehon Ho", "labels": ["pull-request-available"], "created": "2025-10-13T18:57:39.000+0000", "updated": "2025-10-23T23:44:06.000+0000"}, "content": {"description": "SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.  It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values.\r\n\r\nSuggestion from [~aokolnychyi] ", "comments": ["Issue resolved by pull request 52595\n[https://github.com/apache/spark/pull/52595]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53891?", "answer": "SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.  It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values.\r\n\r\nSuggestion from [~aokolnychyi] "}}}
{"metadata": {"id": "13631418", "key": "SPARK-53890", "title": "[SDP] Test (and fix) read/readstream options are respected for pipelines", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Anish Mahto", "assignee": null, "labels": [], "created": "2025-10-13T17:49:49.000+0000", "updated": "2025-10-13T17:52:03.000+0000"}, "content": {"description": "Add tests to verify read/readstream options are actually respected by the flow that executes the read/readstream dataframe.\r\n\r\nTrivial test example might be:\r\n{code:python}\r\n@materialized_view def mv_from_csv():\r\n   return spark.read.option(\"delimiter\", \"|\").csv(\"/my/table.csv\")\r\n{code}\r\nI suspect that today, the read/readstream options will not be respected ([1|https://github.com/apache/spark/blob/master/sql/pipelines/src/main/scala/org/apache/spark/sql/pipelines/graph/FlowAnalysis.scala#L120], [2)|#L131].\r\n\r\nIf true, a solution might be to copy over the options in the `UnresolvedRelation` into either the DataFrameReader that is constructed or the `streamingReadOptions`/`batchReadOptions` argument.\r\n\r\n ", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53890?", "answer": "Add tests to verify read/readstream options are actually respected by the flow that executes the read/readstream dataframe.\r\n\r\nTrivial test example might be:\r\n{code:python}\r\n@materialized_view def mv_from_csv():\r\n   return spark.read.option(\"delimiter\", \"|\").csv(\"/my/table.csv\")\r\n{code}\r\nI suspect that today, the read/readstream options will not be respected ([1|https://github.com/apache/spark/blob/master/sql/pipelines/src/main/scala/org/apache/spark/sql/pipelines/graph/FlowAnalysis.scala#L120], [2)|#L131].\r\n\r\nIf true, a solution might be to copy over the options in the `UnresolvedRelation` into either the DataFrameReader that is constructed or the `streamingReadOptions`/`batchReadOptions` argument.\r\n\r\n "}}}
{"metadata": {"id": "13631416", "key": "SPARK-53889", "title": "Introduce APPROX_PERCENTILE_COMBINE", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-13T17:43:41.000+0000", "updated": "2025-10-24T05:13:00.000+0000"}, "content": {"description": "*APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53889?", "answer": "*APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation."}}}
{"metadata": {"id": "13631415", "key": "SPARK-53888", "title": "Introduce APPROX_PERCENTILE_ESTIMATE", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-13T17:43:22.000+0000", "updated": "2025-10-24T05:13:00.000+0000"}, "content": {"description": "*APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53888?", "answer": "*APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state."}}}
{"metadata": {"id": "13631414", "key": "SPARK-53887", "title": "Introduce APPROX_PERCENTILE_ACCUMULATE", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-13T17:43:04.000+0000", "updated": "2025-10-24T05:13:00.000+0000"}, "content": {"description": "*APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53887?", "answer": "*APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm."}}}
{"metadata": {"id": "13631412", "key": "SPARK-53886", "title": "Percentile estimation functions", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-13T17:42:15.000+0000", "updated": "2025-10-24T05:13:00.000+0000"}, "content": {"description": "Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "comments": ["cc [~cboumalh] [~yhuang95] — would you be interested in taking on these new functions?", "[~Gengliang.Wang] I can take these on.", "[~Gengliang.Wang] Are we interested in creating something like this [https://github.com/apache/datasketches-hive], but for spark? Basically introducing sketch dependencies through connector repo instead of adding them directly to the spark codebase.", "[~cboumalh] Thank for taking this! \r\n\r\n \r\n\r\n> Are we interested in creating something like this [https://github.com/apache/datasketches-hive], but for spark?\r\n\r\n \r\n\r\nNot for now. I think we have already introduced related dependency. This project is to add 3 new functions."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53886?", "answer": "Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation."}}}
{"metadata": {"id": "13631411", "key": "SPARK-53885", "title": "Frequency estimation functions", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Yuchuan Huang", "labels": [], "created": "2025-10-13T17:28:44.000+0000", "updated": "2025-10-24T22:18:14.000+0000"}, "content": {"description": "Introduce the following frequency estimation functions:\r\n * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])* – Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm.\r\n\r\n * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])* – Creates a state object that accumulates frequency statistics for use in later estimation or combination.\r\n\r\n * *APPROX_TOP_K_ESTIMATE(state [, k] ])* – Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state.\r\n\r\n * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* – Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.\r\n\r\n ", "comments": ["To improve the visibility of this feature, I linked this to SPARK-51166 and marked as `Resolved` according to the subtasks. I believe we can add more subtasks (if needed) until we release Apache Spark 4.1.0 release. Thank you, [~yhuang95] and [~Gengliang.Wang]."]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53885?", "answer": "Introduce the following frequency estimation functions:\r\n * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])* – Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm.\r\n\r\n * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])* – Creates a state object that accumulates frequency statistics for use in later estimation or combination.\r\n\r\n * *APPROX_TOP_K_ESTIMATE(state [, k] ])* – Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state.\r\n\r\n * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* – Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.\r\n\r\n "}}}
{"metadata": {"id": "13631399", "key": "SPARK-53884", "title": "Upgrade `ZSTD-JNI` to 1.5.7-5", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-13T15:31:42.000+0000", "updated": "2025-10-13T18:05:13.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52591\n[https://github.com/apache/spark/pull/52591]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53884?", "answer": "No description available."}}}
{"metadata": {"id": "13631385", "key": "SPARK-53883", "title": "SparkConnectClient defines every error class", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Garland Zhang", "assignee": null, "labels": [], "created": "2025-10-13T12:24:50.000+0000", "updated": "2025-10-13T12:24:50.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53883?", "answer": "No description available."}}}
{"metadata": {"id": "13631358", "key": "SPARK-53882", "title": "Add documentation comparing behavioral differences between Spark Connect and Spark Classic", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Xi Lyu", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-13T08:23:51.000+0000", "updated": "2025-10-14T12:57:42.000+0000"}, "content": {"description": "While https://spark.apache.org/docs/latest/spark-connect-overview.html introduces the architecture of Spark Connect, we should add a guide that helps users migrating from Spark Classic understand and avoid unexpected behaviors caused by deferred schema analysis and name resolution.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53882?", "answer": "While https://spark.apache.org/docs/latest/spark-connect-overview.html introduces the architecture of Spark Connect, we should add a guide that helps users migrating from Spark Classic understand and avoid unexpected behaviors caused by deferred schema analysis and name resolution."}}}
{"metadata": {"id": "13631329", "key": "SPARK-53881", "title": "Upgrade `Selenium` to 4.32.0", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-12T18:45:03.000+0000", "updated": "2025-10-24T15:42:39.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52579\n[https://github.com/apache/spark/pull/52579]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53881?", "answer": "No description available."}}}
{"metadata": {"id": "13631328", "key": "SPARK-53880", "title": "Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "L. C. Hsieh", "assignee": "L. C. Hsieh", "labels": ["pull-request-available"], "created": "2025-10-12T18:22:52.000+0000", "updated": "2025-10-25T04:54:32.000+0000"}, "content": {"description": "This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.", "comments": ["Issue resolved by pull request 52578\n[https://github.com/apache/spark/pull/52578]", "[~dongjoon] Thank you for updating this ticket.", "Thank YOU for the fix. :)"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53880?", "answer": "This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2."}}}
{"metadata": {"id": "13631327", "key": "SPARK-53879", "title": "Upgrade `Ammonite` to 3.0.3", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-12T17:58:53.000+0000", "updated": "2025-10-12T23:53:30.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52577\n[https://github.com/apache/spark/pull/52577]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53879?", "answer": "No description available."}}}
{"metadata": {"id": "13631304", "key": "SPARK-53878", "title": "Fix race condition issue related to ObservedMetrics", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "labels": ["pull-request-available"], "created": "2025-10-11T13:33:43.000+0000", "updated": "2025-10-14T00:41:33.000+0000"}, "content": {"description": "In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n \r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsValues(row)\r\n      name -> values\r\n  } {code}\r\n\r\nThis can cause race condition issues. We can see CI failure caused by this issue.\r\nhttps://github.com/apache/spark/actions/runs/18422173471/job/52497913985\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.181s]: test_observe_with_map_type (pyspark.sql.tests.connect.test_parity_observation.DataFrameObservationParityTests.test_observe_with_map_type)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 228, in wrapper\r\n    lastValue = condition(*args, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_observation.py\", line 226, in test_observe_with_map_type\r\n    assertDataFrameEqual(df, [Row(id=id) for id in range(10)])\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 1098, in assertDataFrameEqual\r\n    actual_list = actual.collect()\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1817, in collect\r\n    table, schema = self._to_table()\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1830, in _to_table\r\n    table, schema, self._execution_info = self._session.client.to_table(\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 946, in to_table\r\n    table, schema, metrics, observed_metrics, _ = self._execute_and_fetch(req, observations)\r\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1642, in _execute_and_fetch\r\n    for response in self._execute_and_fetch_as_iterator(\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1619, in _execute_and_fetch_as_iterator\r\n    self._handle_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1893, in _handle_error\r\n    self._handle_rpc_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1966, in _handle_rpc_error\r\n    raise convert_exception(\r\npyspark.errors.exceptions.connect.IllegalArgumentException: requirement failed\r\n\r\nJVM stacktrace:\r\njava.lang.IllegalArgumentException\r\n\tat scala.Predef$.require(Predef.scala:324)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapData.<init>(ArrayBasedMapData.scala:31)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.build(ArrayBasedMapBuilder.scala:130)\r\n\tat org.apache.spark.sql.catalyst.expressions.CreateMap.eval(complexTypeCreator.scala:260)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:162)\r\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedUnsafeProjection.apply(InterpretedUnsafeProjection.scala:84)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.$anonfun$value$2(AggregatingAccumulator.scala:199)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:162)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.withSQLConf(AggregatingAccumulator.scala:106)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.value(AggregatingAccumulator.scala:188)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec.collectedMetrics(CollectMetricsExec.scala:59)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:111)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:109)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:338)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:334)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1$adapted(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach(AdaptiveSparkPlanHelper.scala:45)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach$(AdaptiveSparkPlanHelper.scala:44)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.foreach(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect$(AdaptiveSparkPlanHelper.scala:83)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collectWithSubqueries$1(AdaptiveSparkPlanHelper.scala:113)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries(AdaptiveSparkPlanHelper.scala:113)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries$(AdaptiveSparkPlanHelper.scala:112)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collectWithSubqueries(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:109)\r\n\tat org.apache.spark.sql.execution.QueryExecution.observedMetrics(QueryExecution.scala:276)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.createObservedMetricsResponse(SparkConnectPlanExecution.scala:322)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:82)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:394)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:394)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:113)\r\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:184)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:103)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:393)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:333)\r\n{code}", "comments": ["Issue resolved by pull request 52575\n[https://github.com/apache/spark/pull/52575]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53878?", "answer": "In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n \r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsValues(row)\r\n      name -> values\r\n  } {code}\r\n\r\nThis can cause race condition issues. We can see CI failure caused by this issue.\r\nhttps://github.com/apache/spark/actions/runs/18422173471/job/52497913985\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.181s]: test_observe_with_map_type (pyspark.sql.tests.connect.test_parity_observation.DataFrameObservationParityTests.test_observe_with_map_type)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 228, in wrapper\r\n    lastValue = condition(*args, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_observation.py\", line 226, in test_observe_with_map_type\r\n    assertDataFrameEqual(df, [Row(id=id) for id in range(10)])\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 1098, in assertDataFrameEqual\r\n    actual_list = actual.collect()\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1817, in collect\r\n    table, schema = self._to_table()\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1830, in _to_table\r\n    table, schema, self._execution_info = self._session.client.to_table(\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 946, in to_table\r\n    table, schema, metrics, observed_metrics, _ = self._execute_and_fetch(req, observations)\r\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1642, in _execute_and_fetch\r\n    for response in self._execute_and_fetch_as_iterator(\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1619, in _execute_and_fetch_as_iterator\r\n    self._handle_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1893, in _handle_error\r\n    self._handle_rpc_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1966, in _handle_rpc_error\r\n    raise convert_exception(\r\npyspark.errors.exceptions.connect.IllegalArgumentException: requirement failed\r\n\r\nJVM stacktrace:\r\njava.lang.IllegalArgumentException\r\n\tat scala.Predef$.require(Predef.scala:324)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapData.<init>(ArrayBasedMapData.scala:31)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.build(ArrayBasedMapBuilder.scala:130)\r\n\tat org.apache.spark.sql.catalyst.expressions.CreateMap.eval(complexTypeCreator.scala:260)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:162)\r\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedUnsafeProjection.apply(InterpretedUnsafeProjection.scala:84)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.$anonfun$value$2(AggregatingAccumulator.scala:199)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:162)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.withSQLConf(AggregatingAccumulator.scala:106)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.value(AggregatingAccumulator.scala:188)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec.collectedMetrics(CollectMetricsExec.scala:59)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:111)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:109)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:338)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:334)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1$adapted(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach(AdaptiveSparkPlanHelper.scala:45)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach$(AdaptiveSparkPlanHelper.scala:44)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.foreach(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect$(AdaptiveSparkPlanHelper.scala:83)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collectWithSubqueries$1(AdaptiveSparkPlanHelper.scala:113)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries(AdaptiveSparkPlanHelper.scala:113)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries$(AdaptiveSparkPlanHelper.scala:112)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collectWithSubqueries(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:109)\r\n\tat org.apache.spark.sql.execution.QueryExecution.observedMetrics(QueryExecution.scala:276)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.createObservedMetricsResponse(SparkConnectPlanExecution.scala:322)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:82)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:394)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:394)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:113)\r\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:184)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:103)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:393)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:333)\r\n{code}"}}}
{"metadata": {"id": "13631302", "key": "SPARK-53877", "title": "Add bitmap_and_agg aggregation function", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Uros Stojkovic", "assignee": "Uros Stojkovic", "labels": ["pull-request-available"], "created": "2025-10-11T11:58:35.000+0000", "updated": "2025-10-15T20:36:08.000+0000"}, "content": {"description": "Introduce a function analogous to bitmap_or_agg, but performing a bitwise AND operation instead of OR.\r\n\r\nSpecifically, the bitmap_and_agg function should output a bitmap that represents the bitwise AND of all bitmaps in the input column. The input column must contain bitmaps generated from bitmap_construct_agg(). \r\n\r\nExample:\r\n{code:java}\r\n>>> from pyspark.sql import functions as sf\r\n>>> df = spark.createDataFrame([(\"30\",),(\"70\",),(\"F0\",)], [\"a\"])\r\n>>> df.select(sf.bitmap_and_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\r\n\r\n+--------------------------------+\r\n|bitmap_and_agg(to_binary(a, hex))|\r\n+--------------------------------+\r\n|            [30 00 00 00 00 0...|\r\n+--------------------------------+{code}", "comments": ["Issue resolved by pull request 52586\n[https://github.com/apache/spark/pull/52586]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53877?", "answer": "Introduce a function analogous to bitmap_or_agg, but performing a bitwise AND operation instead of OR.\r\n\r\nSpecifically, the bitmap_and_agg function should output a bitmap that represents the bitwise AND of all bitmaps in the input column. The input column must contain bitmaps generated from bitmap_construct_agg(). \r\n\r\nExample:\r\n{code:java}\r\n>>> from pyspark.sql import functions as sf\r\n>>> df = spark.createDataFrame([(\"30\",),(\"70\",),(\"F0\",)], [\"a\"])\r\n>>> df.select(sf.bitmap_and_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\r\n\r\n+--------------------------------+\r\n|bitmap_and_agg(to_binary(a, hex))|\r\n+--------------------------------+\r\n|            [30 00 00 00 00 0...|\r\n+--------------------------------+{code}"}}}
{"metadata": {"id": "13631289", "key": "SPARK-53876", "title": "Addition of column-level Parquet compression preference in Spark", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Prajwal H G", "assignee": null, "labels": ["compression"], "created": "2025-10-11T06:43:03.000+0000", "updated": "2025-10-11T06:43:03.000+0000"}, "content": {"description": "h4. *Problem*\r\n\r\nApache Spark currently allows only *global compression configuration* for Parquet files using:\r\n\r\n{{spark.sql.parquet.compression.codec = snappy | gzip | zstd | uncompressed}}\r\n\r\nHowever, many production datasets contain heterogeneous columns — for example:\r\n * text or categorical columns that compress better with {*}ZSTD{*},\r\n\r\n * numeric columns that perform better with {*}SNAPPY{*}.\r\n\r\nToday, Spark applies a single codec to the entire file, preventing users from optimizing storage and I/O performance per column.\r\n\r\n\r\nh4. Proposed Improvement\r\n\r\nIntroduce a new configuration key to define *per-column compression codecs* in a map format:\r\n\r\n{{spark.sql.parquet.column.compression.map = colA:zstd,colB:snappy,colC:gzip}}\r\n\r\n*Behavior:*\r\n * The global codec ({{{}spark.sql.parquet.compression.codec{}}}) remains the default for all columns.\r\n\r\n * Any column listed in {{spark.sql.parquet.column.compression.map}} will use its specified codec.\r\n\r\n * Unspecified columns continue to use the global codec.\r\n\r\n*Example:*\r\n\r\n{{--conf spark.sql.parquet.compression.codec=snappy \\}}\r\n\r\n{{--conf spark.sql.parquet.column.compression.map=\"country:zstd,price:snappy,comment:gzip\"\r\n\r\nEffect:-\r\n\r\n}}\r\n||Column||Codec||\r\n|country|zstd|\r\n|price|snappy|\r\n|comment|gzip|\r\n|all others|snappy (global default)|\r\n\r\n{{}}\r\n\r\n ", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53876?", "answer": "h4. *Problem*\r\n\r\nApache Spark currently allows only *global compression configuration* for Parquet files using:\r\n\r\n{{spark.sql.parquet.compression.codec = snappy | gzip | zstd | uncompressed}}\r\n\r\nHowever, many production datasets contain heterogeneous columns — for example:\r\n * text or categorical columns that compress better with {*}ZSTD{*},\r\n\r\n * numeric columns that perform better with {*}SNAPPY{*}.\r\n\r\nToday, Spark applies a single codec to the entire file, preventing users from optimizing storage and I/O performance per column.\r\n\r\n\r\nh4. Proposed Improvement\r\n\r\nIntroduce a new configuration key to define *per-column compression codecs* in a map format:\r\n\r\n{{spark.sql.parquet.column.compression.map = colA:zstd,colB:snappy,colC:gzip}}\r\n\r\n*Behavior:*\r\n * The global codec ({{{}spark.sql.parquet.compression.codec{}}}) remains the default for all columns.\r\n\r\n * Any column listed in {{spark.sql.parquet.column.compression.map}} will use its specified codec.\r\n\r\n * Unspecified columns continue to use the global codec.\r\n\r\n*Example:*\r\n\r\n{{--conf spark.sql.parquet.compression.codec=snappy \\}}\r\n\r\n{{--conf spark.sql.parquet.column.compression.map=\"country:zstd,price:snappy,comment:gzip\"\r\n\r\nEffect:-\r\n\r\n}}\r\n||Column||Codec||\r\n|country|zstd|\r\n|price|snappy|\r\n|comment|gzip|\r\n|all others|snappy (global default)|\r\n\r\n{{}}\r\n\r\n "}}}
{"metadata": {"id": "13631284", "key": "SPARK-53875", "title": "bit_count results incorrect results for negative byte/short/int values", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "labels": ["correctness", "pull-request-available"], "created": "2025-10-11T04:18:14.000+0000", "updated": "2025-10-11T07:34:56.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53875?", "answer": "No description available."}}}
{"metadata": {"id": "13631278", "key": "SPARK-53874", "title": "`SparkAppDriverConf` should respect `sparkVersion` of `SparkApplication` CRD", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-11T00:04:56.000+0000", "updated": "2025-10-11T07:38:03.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 385\n[https://github.com/apache/spark-kubernetes-operator/pull/385]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53874?", "answer": "No description available."}}}
{"metadata": {"id": "13631274", "key": "SPARK-53873", "title": "ExplodeBase.eval Iterate directly on input", "project": "SPARK", "status": "Open", "priority": "Trivial", "reporter": "Andy Sautins", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-10T21:52:23.000+0000", "updated": "2025-10-24T05:13:00.000+0000"}, "content": {"description": "It was noticed that `ExplodeBase.eval` returns an IterableOnce[InternalRow].  The current implementation creates a pre-allocated array, populates the array appropriately, and returns the Array.  This works as the is an implicit conversion from Array to IterableOnce.\r\n\r\nHowever Allocating and populating an array does not seem to provide benefits over exposing an iterator over the input data type.\r\n\r\nA proposed PR removes the creation and population of the array and instead returns a IterableOnce object that iterates over the underlying input.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53873?", "answer": "It was noticed that `ExplodeBase.eval` returns an IterableOnce[InternalRow].  The current implementation creates a pre-allocated array, populates the array appropriately, and returns the Array.  This works as the is an implicit conversion from Array to IterableOnce.\r\n\r\nHowever Allocating and populating an array does not seem to provide benefits over exposing an iterator over the input data type.\r\n\r\nA proposed PR removes the creation and population of the array and instead returns a IterableOnce object that iterates over the underlying input."}}}
{"metadata": {"id": "13631270", "key": "SPARK-53872", "title": "Revisit `JUnit` assert usage in test cases", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-10T20:50:12.000+0000", "updated": "2025-10-10T21:40:05.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 384\n[https://github.com/apache/spark-kubernetes-operator/pull/384]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53872?", "answer": "No description available."}}}
{"metadata": {"id": "13631267", "key": "SPARK-53871", "title": "Upgrade `JUnit` to 6.0.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-10T19:59:24.000+0000", "updated": "2025-10-10T20:12:30.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 383\n[https://github.com/apache/spark-kubernetes-operator/pull/383]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53871?", "answer": "No description available."}}}
{"metadata": {"id": "13631262", "key": "SPARK-53870", "title": "Python streaming transform_with_state StateServer does not fully read large state values", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jason Teoh", "assignee": "Jason Teoh", "labels": ["pull-request-available"], "created": "2025-10-10T19:05:20.000+0000", "updated": "2025-10-14T02:22:17.000+0000"}, "content": {"description": "The TransformWithState StateServer's {{parseProtoMessage}} method uses {{read}} (InputStream/FilterInputStream) which only reads all available data and may not return the full message. We should be using the [readFully DataInputStream API|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/DataInput.html#readFully(byte%5B%5D)] instead, which will continue fetching until it fills up the provided buffer.\r\n\r\nIn addition to the linked API above, this StackOverflow post also illustrates the difference between the two APIs: [https://stackoverflow.com/a/25900095]\r\n\r\nWithout this change, it is possible for the state server to fail to fully read large proto messages (e.g., those containing a large state value update) and run into a parsing error.\r\n\r\n \r\n\r\nAffected versions identified by the tags on the original PR, it seems to have been present since the state server was introduced: [https://github.com/apache/spark/commit/def42d44405af5df78c3039ac5ad0f8a0469efaa]\r\n\r\n \r\n\r\nIn practice this seems like an uncommon scenario (bug was identified/confirmed with a 512KB string state value update which likely produces a proto message much larger than typical use cases)\r\n\r\n ", "comments": ["Issue resolved via [https://github.com/apache/spark/pull/52539]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53870?", "answer": "The TransformWithState StateServer's {{parseProtoMessage}} method uses {{read}} (InputStream/FilterInputStream) which only reads all available data and may not return the full message. We should be using the [readFully DataInputStream API|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/DataInput.html#readFully(byte%5B%5D)] instead, which will continue fetching until it fills up the provided buffer.\r\n\r\nIn addition to the linked API above, this StackOverflow post also illustrates the difference between the two APIs: [https://stackoverflow.com/a/25900095]\r\n\r\nWithout this change, it is possible for the state server to fail to fully read large proto messages (e.g., those containing a large state value update) and run into a parsing error.\r\n\r\n \r\n\r\nAffected versions identified by the tags on the original PR, it seems to have been present since the state server was introduced: [https://github.com/apache/spark/commit/def42d44405af5df78c3039ac5ad0f8a0469efaa]\r\n\r\n \r\n\r\nIn practice this seems like an uncommon scenario (bug was identified/confirmed with a 512KB string state value update which likely produces a proto message much larger than typical use cases)\r\n\r\n "}}}
{"metadata": {"id": "13631260", "key": "SPARK-53869", "title": "Support multiple files in `pyFiles`", "project": "SPARK", "status": "Resolved", "priority": "Critical", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-10T18:17:38.000+0000", "updated": "2025-10-10T18:48:56.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 382\n[https://github.com/apache/spark-kubernetes-operator/pull/382]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53869?", "answer": "No description available."}}}
{"metadata": {"id": "13631252", "key": "SPARK-53868", "title": "Block unsupported aggregates in both signatures of `visitAggregateFunction`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Alek Jarmov", "assignee": "Alek Jarmov", "labels": ["pull-request-available"], "created": "2025-10-10T16:19:04.000+0000", "updated": "2025-10-13T15:14:14.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52573\n[https://github.com/apache/spark/pull/52573]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53868?", "answer": "No description available."}}}
{"metadata": {"id": "13631236", "key": "SPARK-53867", "title": "Limit Arrow batch sizes in SQL_GROUPED_AGG_ARROW_UDF", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-10T13:48:19.000+0000", "updated": "2025-10-15T03:55:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52605\n[https://github.com/apache/spark/pull/52605]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53867?", "answer": "No description available."}}}
{"metadata": {"id": "13631229", "key": "SPARK-53866", "title": "Skip doctest `pyspark.sql.pandas.functions` without pyarrow/pandas", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-10T12:24:16.000+0000", "updated": "2025-10-10T18:59:39.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52572\n[https://github.com/apache/spark/pull/52572]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53866?", "answer": "No description available."}}}
{"metadata": {"id": "13631225", "key": "SPARK-53865", "title": "Extract common Generate resolution logic", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Mikhail Nikoliukin", "assignee": "Mikhail Nikoliukin", "labels": ["pull-request-available"], "created": "2025-10-10T11:05:47.000+0000", "updated": "2025-10-10T16:02:40.000+0000"}, "content": {"description": "I'm planning to add Generate node resolution to the single-pass analyzer. Before that, I need to do minor refactoring to extract common logic from ResolveGenerate rule.", "comments": ["Issue resolved by pull request 52571\n[https://github.com/apache/spark/pull/52571]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53865?", "answer": "I'm planning to add Generate node resolution to the single-pass analyzer. Before that, I need to do minor refactoring to extract common logic from ResolveGenerate rule."}}}
{"metadata": {"id": "13631202", "key": "SPARK-53864", "title": "Upgrade commons-lang3 to 3.19.0 ", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "labels": ["pull-request-available"], "created": "2025-10-10T06:36:18.000+0000", "updated": "2025-10-11T07:37:06.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52485\n[https://github.com/apache/spark/pull/52485]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53864?", "answer": "No description available."}}}
{"metadata": {"id": "13631196", "key": "SPARK-53863", "title": "Remove `Java 17` requirement from `deploy.gradle`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-10T03:24:19.000+0000", "updated": "2025-10-10T03:51:19.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 381\n[https://github.com/apache/spark-kubernetes-operator/pull/381]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53863?", "answer": "No description available."}}}
{"metadata": {"id": "13631194", "key": "SPARK-53862", "title": "Fix `CheckpointSuite.'get correct spark.driver.[host|port] from checkpoint'` test flakiness", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-10T02:02:21.000+0000", "updated": "2025-10-10T02:15:04.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52565\n[https://github.com/apache/spark/pull/52565]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53862?", "answer": "No description available."}}}
{"metadata": {"id": "13631192", "key": "SPARK-53861", "title": "Factor out streaming tests from `spark-sql` and `spark-connect`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-10T00:50:57.000+0000", "updated": "2025-10-10T05:28:21.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52564\n[https://github.com/apache/spark/pull/52564]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53861?", "answer": "No description available."}}}
{"metadata": {"id": "13631188", "key": "SPARK-53860", "title": "Upgrade `sbt-jupiter-interface` to 0.17.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T23:01:57.000+0000", "updated": "2025-10-24T15:42:29.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52562\n[https://github.com/apache/spark/pull/52562]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53860?", "answer": "No description available."}}}
{"metadata": {"id": "13631187", "key": "SPARK-53859", "title": "Upgrade `JUnit` to 6.0.0", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T22:39:09.000+0000", "updated": "2025-10-24T15:36:25.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52561\n[https://github.com/apache/spark/pull/52561]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53859?", "answer": "No description available."}}}
{"metadata": {"id": "13631184", "key": "SPARK-53858", "title": "Skip doctests in pyspark.sql.functions.builtin if pyarrow is not installe", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-09T21:06:53.000+0000", "updated": "2025-10-10T05:12:50.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52569\n[https://github.com/apache/spark/pull/52569]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53858?", "answer": "No description available."}}}
{"metadata": {"id": "13631175", "key": "SPARK-53857", "title": "Enable messageTemplate propagation to SparkThrowable ", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Milan Dankovic", "assignee": "Milan Dankovic", "labels": ["pull-request-available"], "created": "2025-10-09T18:23:47.000+0000", "updated": "2025-10-14T13:52:59.000+0000"}, "content": {"description": "The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message.", "comments": ["I am working on this", "Issue resolved by pull request 52559\n[https://github.com/apache/spark/pull/52559]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53857?", "answer": "The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message."}}}
{"metadata": {"id": "13631161", "key": "SPARK-53856", "title": "Remove `blacklist` alternative config names", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T16:21:41.000+0000", "updated": "2025-10-10T15:12:07.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52558\n[https://github.com/apache/spark/pull/52558]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53856?", "answer": "No description available."}}}
{"metadata": {"id": "13631114", "key": "SPARK-53855", "title": "Fix PlanID preservation in ResolveSQLFunctions with UDFs", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Uroš Bojanić", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-09T09:56:02.000+0000", "updated": "2025-10-10T13:06:08.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53855?", "answer": "No description available."}}}
{"metadata": {"id": "13631088", "key": "SPARK-53854", "title": "Skip `test_collect_time` test if pandas or pyarrow are unavailable", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T05:36:43.000+0000", "updated": "2025-10-09T16:50:29.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52555\n[https://github.com/apache/spark/pull/52555]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53854?", "answer": "No description available."}}}
{"metadata": {"id": "13631087", "key": "SPARK-53853", "title": "Add `Example` section in `operators.md`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T05:09:47.000+0000", "updated": "2025-10-09T09:12:58.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 380\n[https://github.com/apache/spark-kubernetes-operator/pull/380]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53853?", "answer": "No description available."}}}
{"metadata": {"id": "13631085", "key": "SPARK-53852", "title": "Add `spark.kubernetes.driver.pod.excludedFeatureSteps` usage example", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T04:13:04.000+0000", "updated": "2025-10-09T06:59:36.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 379\n[https://github.com/apache/spark-kubernetes-operator/pull/379]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53852?", "answer": "No description available."}}}
{"metadata": {"id": "13631083", "key": "SPARK-53851", "title": "Set `io.netty.noUnsafe` to `true` to avoid JEP-498 warnings", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-09T03:36:08.000+0000", "updated": "2025-10-09T06:58:56.000+0000"}, "content": {"description": "\r\n*BEFORE*\r\n{code}\r\nStarting Operator...\r\nWARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\nWARNING: sun.misc.Unsafe::allocateMemory has been called by io.netty.util.internal.PlatformDependent0$2 (file:/opt/spark-operator/operator/spark-kubernetes-operator.jar)\r\nWARNING: Please consider reporting this to the maintainers of class io.netty.util.internal.PlatformDependent0$2\r\nWARNING: sun.misc.Unsafe::allocateMemory will be removed in a future release\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n{code}\r\n\r\n*AFTER*\r\n{code}\r\nStarting Operator...\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.                                                                    │\r\n{code}", "comments": ["Issue resolved by pull request 378\n[https://github.com/apache/spark-kubernetes-operator/pull/378]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53851?", "answer": "\r\n*BEFORE*\r\n{code}\r\nStarting Operator...\r\nWARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\nWARNING: sun.misc.Unsafe::allocateMemory has been called by io.netty.util.internal.PlatformDependent0$2 (file:/opt/spark-operator/operator/spark-kubernetes-operator.jar)\r\nWARNING: Please consider reporting this to the maintainers of class io.netty.util.internal.PlatformDependent0$2\r\nWARNING: sun.misc.Unsafe::allocateMemory will be removed in a future release\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n{code}\r\n\r\n*AFTER*\r\n{code}\r\nStarting Operator...\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.                                                                    │\r\n{code}"}}}
{"metadata": {"id": "13631069", "key": "SPARK-53850", "title": "Define Proto for Sinks", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jacky Wang", "assignee": "Jacky Wang", "labels": ["pull-request-available"], "created": "2025-10-08T23:24:49.000+0000", "updated": "2025-10-09T22:35:24.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52553\n[https://github.com/apache/spark/pull/52553]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53850?", "answer": "No description available."}}}
{"metadata": {"id": "13631068", "key": "SPARK-53849", "title": "Upgrade Netty to 4.2.6.Final", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T22:21:01.000+0000", "updated": "2025-10-24T05:16:55.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52552\n[https://github.com/apache/spark/pull/52552]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53849?", "answer": "No description available."}}}
{"metadata": {"id": "13631066", "key": "SPARK-53848", "title": "Support for Sketch family in ThetaSketch Aggregates", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Karuppayya", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-08T21:45:11.000+0000", "updated": "2025-10-15T02:28:59.000+0000"}, "content": {"description": "Theta sketch aggregate currently supports only quick select.\r\n\r\nConsumers like Iceberg{^}[1][2]{^} might benefit will benefit from the sketch aggregate if has the ability to specify `ALPHA family`\r\n\r\n[1] [Iceberg specification to use ALPHA sketches|https://iceberg.apache.org/puffin-spec/#apache-datasketches-theta-v1-blob-type]\r\n\r\n[2] [Custom implementation of theta sketch aggregates in Iceberg|https://github.com/apache/iceberg/blob/2f6e7e6371902bcb72f21deeaea8889d4768004e/spark/v3.5/spark/src/main/scala/org/apache/spark/sql/stats/ThetaSketchAgg.scala#L67] that can be replaced with Spark Theta aggregates", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53848?", "answer": "Theta sketch aggregate currently supports only quick select.\r\n\r\nConsumers like Iceberg{^}[1][2]{^} might benefit will benefit from the sketch aggregate if has the ability to specify `ALPHA family`\r\n\r\n[1] [Iceberg specification to use ALPHA sketches|https://iceberg.apache.org/puffin-spec/#apache-datasketches-theta-v1-blob-type]\r\n\r\n[2] [Custom implementation of theta sketch aggregates in Iceberg|https://github.com/apache/iceberg/blob/2f6e7e6371902bcb72f21deeaea8889d4768004e/spark/v3.5/spark/src/main/scala/org/apache/spark/sql/stats/ThetaSketchAgg.scala#L67] that can be replaced with Spark Theta aggregates"}}}
{"metadata": {"id": "13631063", "key": "SPARK-53847", "title": "Add ContinuousMemorySink for RTM testing", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "labels": ["pull-request-available"], "created": "2025-10-08T20:43:28.000+0000", "updated": "2025-10-24T05:10:48.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52550\n[https://github.com/apache/spark/pull/52550]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53847?", "answer": "No description available."}}}
{"metadata": {"id": "13631061", "key": "SPARK-53846", "title": "Skip `test_profile_pandas_*` tests if pandas or pyarrow are unavailable", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T20:36:42.000+0000", "updated": "2025-10-09T01:37:36.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52549\n[https://github.com/apache/spark/pull/52549]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53846?", "answer": "No description available."}}}
{"metadata": {"id": "13631058", "key": "SPARK-53845", "title": "[SDP] Sinks", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Jacky Wang", "assignee": "Jacky Wang", "labels": ["pull-request-available"], "created": "2025-10-08T19:57:12.000+0000", "updated": "2025-10-13T17:07:26.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52563\n[https://github.com/apache/spark/pull/52563]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53845?", "answer": "No description available."}}}
{"metadata": {"id": "13631056", "key": "SPARK-53844", "title": "Remove `SPARK_JENKINS` and related logics from `dev/run-tests.py`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T19:45:54.000+0000", "updated": "2025-10-08T22:15:07.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52548\n[https://github.com/apache/spark/pull/52548]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53844?", "answer": "No description available."}}}
{"metadata": {"id": "13631033", "key": "SPARK-53843", "title": "Upgrade `netty-tcnative` to 2.0.74.Final", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T17:19:17.000+0000", "updated": "2025-10-24T05:19:37.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52547\n[https://github.com/apache/spark/pull/52547]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53843?", "answer": "No description available."}}}
{"metadata": {"id": "13631019", "key": "SPARK-53842", "title": " Enable Filter Push-Down for Pandas UDFs with an Immutable Column Hint", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Lior Itzhak", "assignee": null, "labels": [], "created": "2025-10-08T15:17:42.000+0000", "updated": "2025-10-11T10:34:28.000+0000"}, "content": {"description": "h3. Problem Description\r\n\r\nPandas UDFs ({{{}mapInPandas{}}}, {{{}applyInPandas{}}}, etc.) are powerful for custom data processing in PySpark. However, they currently act as a black box to the Catalyst Optimizer. This prevents the optimizer from pushing down filters on columns that pass through the UDF unmodified. As a result, filtering operations occur _after_ the expensive UDF execution and associated data shuffling, leading to significant performance degradation.\r\n\r\nThis is especially common in pipelines where transformations are applied to grouped data, and the grouping key itself is not modified within the UDF.\r\n\r\n*Example:*\r\n\r\nConsider the following DataFrame and Pandas UDFs:\r\n{code:java}\r\nimport pandas as pd\r\nfrom typing import Iterator\r\n\r\ndf = spark.createDataFrame(\r\n    [[\"A\", 1], [\"A\", 1], [\"B\", 2]], \r\n    schema=[\"id string\", \"value int\"]\r\n)\r\n\r\n# UDF to modify the 'value' column\r\ndef map_udf(pdfs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\r\n    for pdf in pdfs:\r\n        pdf[\"value\"] = pdf[\"value\"] + 1\r\n        yield pdf\r\n\r\n# UDF to aggregate data by 'id'\r\ndef agg_udf(pdf: pd.DataFrame) -> pd.DataFrame:\r\n    return pdf.groupby(\"id\").agg(count=(\"value\", \"count\"))\r\n\r\n# Apply the UDFs\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(map_udf, schema=\"id string,value int\")\r\n    .groupby(\"id\")\r\n    .applyInPandas(agg_udf, schema=\"id string,count int\")\r\n)\r\n\r\n# Filter the result\r\nmodified_df.where(\"id == 'A'\").explain() {code}\r\nIn this example, the {{id}} column is never modified by either UDF. However, the filter on {{id}} is applied only after all transformations are complete.\r\n\r\n*Current Physical Plan:*\r\n\r\nThe physical plan shows the {{Filter}} operation at the very top, processing data that has already been scanned, shuffled, and processed by both Pandas UDFs.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Filter (isnotnull(id#20) AND (id#20 = A))\r\n   +- FlatMapGroupsInPandas [id#13], agg_udf(id#13, value#14)#19, [id#20, count#21]\r\n      +- Sort [id#13 ASC NULLS FIRST], false, 0\r\n         +- Exchange hashpartitioning(id#13, 200), ENSURE_REQUIREMENTS, [plan_id=20]\r\n            +- Project [id#13, id#13, value#14]\r\n               +- MapInPandas map_udf(id string#8, value int#9L)#12, [id#13, value#14], false\r\n                  +- Scan ExistingRDD[id string#8,value int#9L]{code}\r\n{{ }}\r\n\r\nThis plan processes all data for both {{id = 'A'}} and {{id = 'B'}} through the entire pipeline, even though the data for {{'B'}} is discarded at the end.\r\nh3. Proposed Solution\r\n\r\nWe propose introducing a mechanism to *hint* to the Catalyst Optimizer that specific columns within a Pandas UDF are immutable or pass through without modification. This would allow the optimizer to safely push down filters on these columns.\r\n\r\nThis could be implemented as a new parameter in the UDF registration, for example, {{{}passthrough_cols{}}}:\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n# Proposed API modification\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(\r\n        map_udf, \r\n        schema=\"id string,value int\",\r\n        passthrough_cols=[\"id\"]  # New hint parameter\r\n    )\r\n    .groupby(\"id\")\r\n    .applyInPandas(\r\n        agg_udf, \r\n        schema=\"id string,count int\",\r\n        passthrough_cols=[\"id\"]  # New hint parameter\r\n    )\r\n)\r\n{code}\r\n{{ }}\r\n\r\nWith this hint, the optimizer could transform the physical plan to apply the filter at the data source, _before_ any expensive operations.\r\n\r\n*Desired Physical Plan:*\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n== Desired Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- FlatMapGroupsInPandas [id#13], agg_udf(id#13, value#14)#19, [id#20, count#21]\r\n   +- Sort [id#13 ASC NULLS FIRST], false, 0\r\n      +- Exchange hashpartitioning(id#13, 200), ENSURE_REQUIREMENTS, [plan_id=20]\r\n         +- Project [id#13, id#13, value#14]\r\n            +- MapInPandas map_udf(id string#8, value int#9L)#12, [id#13, value#14], false\r\n               +- Filter (isnotnull(id#8) AND (id#8 = A))  // <-- FILTER PUSHED DOWN\r\n                  +- Scan ExistingRDD[id string#8,value int#9L]]{code}\r\n{{ }}\r\n\r\nThis optimized plan would significantly reduce the amount of data sent to the UDFs and shuffled across the network, resulting in major performance improvements.\r\nh3. Motivation & Justification\r\n # *Performance:* In large-scale data processing pipelines, filtering data early is one of the most effective optimization strategies. Enabling filter push-down for Pandas UDFs would unlock substantial performance gains, reducing I/O, network traffic, and computational load.\r\n\r\n # *Common Use Case:* Developers often know with certainty that grouping keys or other identifier columns are not modified within their UDFs. The proposed hint provides a direct means of communicating this domain knowledge to the optimizer.\r\n\r\n # *Usability:* This feature would empower developers to optimize their pipelines in scenarios where they cannot change an incoming plan and can only apply transformations to a given DataFrame.\r\n\r\nh3. Optional: Runtime Validation\r\n\r\nTo safeguard against incorrect usage of the hint, Spark could optionally perform a runtime validation. This check would verify that the values in the columns marked as {{passthrough_cols}} are indeed unchanged between the input and output of the UDF. If a discrepancy is found (e.g., a value in the output {{id}} column did not exist in the input {{id}} column for that batch), Spark could raise an exception. While not entirely foolproof, this would cover most grouping and mapping use cases and prevent subtle bugs.", "comments": ["Hi [~lioritzhak] ,\r\n\r\nI would _love_ to work on this, because I am super excited about query optimization and want to get my feet wet working on Spark. Since this would be my first issue for Spark, I might have questions during the process, though.\r\nI do have another thing to work on first, AND I'll have to setup my dev eivironment for Spark first (which might take some time). All in all, I might take 2 to 3 weeks for setting this up and getting ready, I'd guess (subject to change!).\r\nIf that does not sound too off-putting, feel free to assign this to me. I'd be super excited to do this!\r\n\r\nBest,\r\nJulian"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53842?", "answer": "h3. Problem Description\r\n\r\nPandas UDFs ({{{}mapInPandas{}}}, {{{}applyInPandas{}}}, etc.) are powerful for custom data processing in PySpark. However, they currently act as a black box to the Catalyst Optimizer. This prevents the optimizer from pushing down filters on columns that pass through the UDF unmodified. As a result, filtering operations occur _after_ the expensive UDF execution and associated data shuffling, leading to significant performance degradation.\r\n\r\nThis is especially common in pipelines where transformations are applied to grouped data, and the grouping key itself is not modified within the UDF.\r\n\r\n*Example:*\r\n\r\nConsider the following DataFrame and Pandas UDFs:\r\n{code:java}\r\nimport pandas as pd\r\nfrom typing import Iterator\r\n\r\ndf = spark.createDataFrame(\r\n    [[\"A\", 1], [\"A\", 1], [\"B\", 2]], \r\n    schema=[\"id string\", \"value int\"]\r\n)\r\n\r\n# UDF to modify the 'value' column\r\ndef map_udf(pdfs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\r\n    for pdf in pdfs:\r\n        pdf[\"value\"] = pdf[\"value\"] + 1\r\n        yield pdf\r\n\r\n# UDF to aggregate data by 'id'\r\ndef agg_udf(pdf: pd.DataFrame) -> pd.DataFrame:\r\n    return pdf.groupby(\"id\").agg(count=(\"value\", \"count\"))\r\n\r\n# Apply the UDFs\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(map_udf, schema=\"id string,value int\")\r\n    .groupby(\"id\")\r\n    .applyInPandas(agg_udf, schema=\"id string,count int\")\r\n)\r\n\r\n# Filter the result\r\nmodified_df.where(\"id == 'A'\").explain() {code}\r\nIn this example, the {{id}} column is never modified by either UDF. However, the filter on {{id}} is applied only after all transformations are complete.\r\n\r\n*Current Physical Plan:*\r\n\r\nThe physical plan shows the {{Filter}} operation at the very top, processing data that has already been scanned, shuffled, and processed by both Pandas UDFs.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Filter (isnotnull(id#20) AND (id#20 = A))\r\n   +- FlatMapGroupsInPandas [id#13], agg_udf(id#13, value#14)#19, [id#20, count#21]\r\n      +- Sort [id#13 ASC NULLS FIRST], false, 0\r\n         +- Exchange hashpartitioning(id#13, 200), ENSURE_REQUIREMENTS, [plan_id=20]\r\n            +- Project [id#13, id#13, value#14]\r\n               +- MapInPandas map_udf(id string#8, value int#9L)#12, [id#13, value#14], false\r\n                  +- Scan ExistingRDD[id string#8,value int#9L]{code}\r\n{{ }}\r\n\r\nThis plan processes all data for both {{id = 'A'}} and {{id = 'B'}} through the entire pipeline, even though the data for {{'B'}} is discarded at the end.\r\nh3. Proposed Solution\r\n\r\nWe propose introducing a mechanism to *hint* to the Catalyst Optimizer that specific columns within a Pandas UDF are immutable or pass through without modification. This would allow the optimizer to safely push down filters on these columns.\r\n\r\nThis could be implemented as a new parameter in the UDF registration, for example, {{{}passthrough_cols{}}}:\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n# Proposed API modification\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(\r\n        map_udf, \r\n        schema=\"id string,value int\",\r\n        passthrough_cols=[\"id\"]  # New hint parameter\r\n    )\r\n    .groupby(\"id\")\r\n    .applyInPandas(\r\n        agg_udf, \r\n        schema=\"id string,count int\",\r\n        passthrough_cols=[\"id\"]  # New hint parameter\r\n    )\r\n)\r\n{code}\r\n{{ }}\r\n\r\nWith this hint, the optimizer could transform the physical plan to apply the filter at the data source, _before_ any expensive operations.\r\n\r\n*Desired Physical Plan:*\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n== Desired Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- FlatMapGroupsInPandas [id#13], agg_udf(id#13, value#14)#19, [id#20, count#21]\r\n   +- Sort [id#13 ASC NULLS FIRST], false, 0\r\n      +- Exchange hashpartitioning(id#13, 200), ENSURE_REQUIREMENTS, [plan_id=20]\r\n         +- Project [id#13, id#13, value#14]\r\n            +- MapInPandas map_udf(id string#8, value int#9L)#12, [id#13, value#14], false\r\n               +- Filter (isnotnull(id#8) AND (id#8 = A))  // <-- FILTER PUSHED DOWN\r\n                  +- Scan ExistingRDD[id string#8,value int#9L]]{code}\r\n{{ }}\r\n\r\nThis optimized plan would significantly reduce the amount of data sent to the UDFs and shuffled across the network, resulting in major performance improvements.\r\nh3. Motivation & Justification\r\n # *Performance:* In large-scale data processing pipelines, filtering data early is one of the most effective optimization strategies. Enabling filter push-down for Pandas UDFs would unlock substantial performance gains, reducing I/O, network traffic, and computational load.\r\n\r\n # *Common Use Case:* Developers often know with certainty that grouping keys or other identifier columns are not modified within their UDFs. The proposed hint provides a direct means of communicating this domain knowledge to the optimizer.\r\n\r\n # *Usability:* This feature would empower developers to optimize their pipelines in scenarios where they cannot change an incoming plan and can only apply transformations to a given DataFrame.\r\n\r\nh3. Optional: Runtime Validation\r\n\r\nTo safeguard against incorrect usage of the hint, Spark could optionally perform a runtime validation. This check would verify that the values in the columns marked as {{passthrough_cols}} are indeed unchanged between the input and output of the UDF. If a discrepancy is found (e.g., a value in the output {{id}} column did not exist in the input {{id}} column for that batch), Spark could raise an exception. While not entirely foolproof, this would cover most grouping and mapping use cases and prevent subtle bugs."}}}
{"metadata": {"id": "13631016", "key": "SPARK-53841", "title": "Implement transform in column API in PySpark", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yicong Huang", "assignee": "Yicong Huang", "labels": ["pull-request-available"], "created": "2025-10-08T15:10:01.000+0000", "updated": "2025-10-14T08:39:35.000+0000"}, "content": {"description": "related to https://issues.apache.org/jira/browse/SPARK-53779", "comments": ["I will work on this.", "Issue resolved by pull request 52593\n[https://github.com/apache/spark/pull/52593]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53841?", "answer": "related to https://issues.apache.org/jira/browse/SPARK-53779"}}}
{"metadata": {"id": "13631013", "key": "SPARK-53840", "title": "Show Table As JSON", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Amanda Liu", "assignee": null, "labels": [], "created": "2025-10-08T14:26:29.000+0000", "updated": "2025-10-08T14:26:29.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53840?", "answer": "No description available."}}}
{"metadata": {"id": "13631012", "key": "SPARK-53839", "title": "Describe Table As JSON v2 table path", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Amanda Liu", "assignee": null, "labels": [], "created": "2025-10-08T14:22:30.000+0000", "updated": "2025-10-08T14:22:43.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53839?", "answer": "No description available."}}}
{"metadata": {"id": "13631011", "key": "SPARK-53838", "title": "Describe Table Col As JSON", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Amanda Liu", "assignee": null, "labels": [], "created": "2025-10-08T14:22:06.000+0000", "updated": "2025-10-08T14:22:06.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53838?", "answer": "No description available."}}}
{"metadata": {"id": "13631010", "key": "SPARK-53837", "title": "Describe As JSON ", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Amanda Liu", "assignee": null, "labels": [], "created": "2025-10-08T14:21:25.000+0000", "updated": "2025-10-08T14:21:25.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53837?", "answer": "No description available."}}}
{"metadata": {"id": "13630993", "key": "SPARK-53836", "title": "Update script `free_disk_space_container`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-08T11:57:11.000+0000", "updated": "2025-10-08T15:15:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52545\n[https://github.com/apache/spark/pull/52545]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53836?", "answer": "No description available."}}}
{"metadata": {"id": "13630953", "key": "SPARK-53835", "title": "Install `pyarrow/mlflow/torch/torchvision` packages to `Python 3.14` Dockefile", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": null, "labels": [], "created": "2025-10-08T06:17:37.000+0000", "updated": "2025-10-09T23:16:28.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53835?", "answer": "No description available."}}}
{"metadata": {"id": "13630948", "key": "SPARK-53834", "title": "Add a separate docker file for Python 3.14 daily build", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T05:37:55.000+0000", "updated": "2025-10-08T23:45:17.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52544\n[https://github.com/apache/spark/pull/52544]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53834?", "answer": "No description available."}}}
{"metadata": {"id": "13630943", "key": "SPARK-53833", "title": "Update `dev/requirements.txt` to skip `torch` and `torchvision` in Python 3.14", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T04:56:22.000+0000", "updated": "2025-10-08T08:32:48.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52543\n[https://github.com/apache/spark/pull/52543]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53833?", "answer": "No description available."}}}
{"metadata": {"id": "13630942", "key": "SPARK-53832", "title": "Make `KubernetesClientUtils` Java-friendly", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T04:04:11.000+0000", "updated": "2025-10-24T15:17:43.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52542\n[https://github.com/apache/spark/pull/52542]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53832?", "answer": "No description available."}}}
{"metadata": {"id": "13630941", "key": "SPARK-53831", "title": "Update script `free_disk_space`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-08T02:40:44.000+0000", "updated": "2025-10-08T04:14:57.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52541\n[https://github.com/apache/spark/pull/52541]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53831?", "answer": "No description available."}}}
{"metadata": {"id": "13630940", "key": "SPARK-53830", "title": "Exclude `.idea` directory from `Spotless` check", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-08T02:21:08.000+0000", "updated": "2025-10-08T02:45:00.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 377\n[https://github.com/apache/spark-kubernetes-operator/pull/377]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53830?", "answer": "No description available."}}}
{"metadata": {"id": "13630938", "key": "SPARK-53829", "title": "Support `datetime.time` in column operators", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "labels": ["pull-request-available"], "created": "2025-10-08T01:35:57.000+0000", "updated": "2025-10-08T04:12:53.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52540\n[https://github.com/apache/spark/pull/52540]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53829?", "answer": "No description available."}}}
{"metadata": {"id": "13630935", "key": "SPARK-53828", "title": "Exclude `org/apache/spark/ui/static/**` from Spark operator jar", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T23:53:10.000+0000", "updated": "2025-10-08T00:13:28.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 376\n[https://github.com/apache/spark-kubernetes-operator/pull/376]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53828?", "answer": "No description available."}}}
{"metadata": {"id": "13630933", "key": "SPARK-53827", "title": "Exclude `commons-(codec|compress)` and `com.ibm.icu` transitive dependencies ", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T23:32:30.000+0000", "updated": "2025-10-08T00:03:39.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 375\n[https://github.com/apache/spark-kubernetes-operator/pull/375]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53827?", "answer": "No description available."}}}
{"metadata": {"id": "13630924", "key": "SPARK-53826", "title": "Use Java 25 in GitHub Action jobs consistently", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T21:28:19.000+0000", "updated": "2025-10-07T22:26:55.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 374\n[https://github.com/apache/spark-kubernetes-operator/pull/374]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53826?", "answer": "No description available."}}}
{"metadata": {"id": "13630922", "key": "SPARK-53825", "title": "Use Java `MessageDigest` instead of `org.apache.commons.codec`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T21:07:50.000+0000", "updated": "2025-10-07T22:24:32.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 373\n[https://github.com/apache/spark-kubernetes-operator/pull/373]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53825?", "answer": "No description available."}}}
{"metadata": {"id": "13630919", "key": "SPARK-53824", "title": "Ban `org.apache.commons.collections4` package", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T20:32:03.000+0000", "updated": "2025-10-07T22:20:12.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 372\n[https://github.com/apache/spark-kubernetes-operator/pull/372]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53824?", "answer": "No description available."}}}
{"metadata": {"id": "13630916", "key": "SPARK-53823", "title": "Allow list for operators", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "labels": [], "created": "2025-10-07T19:19:43.000+0000", "updated": "2025-10-24T05:11:13.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53823?", "answer": "No description available."}}}
{"metadata": {"id": "13630913", "key": "SPARK-53822", "title": "Add python transform_with_state test case for output schemas containing composite StructType/nested StructType like ArrayType/MapType", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Jason Teoh", "assignee": "Jason Teoh", "labels": ["pull-request-available"], "created": "2025-10-07T18:02:20.000+0000", "updated": "2025-10-10T22:13:55.000+0000"}, "content": {"description": "Existing test case `test_transform_with_state_in_pandas_composite_type` does cover composite schemas, but only within the state API.  The output type of that test case is still strings (e.g., dicts are json dumped into a string).\r\n\r\nThis ticket is to add a test case where the output of the transform_with_state operation itself contains nested types.", "comments": ["Issue resolved by pull request 52536\n[https://github.com/apache/spark/pull/52536]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53822?", "answer": "Existing test case `test_transform_with_state_in_pandas_composite_type` does cover composite schemas, but only within the state API.  The output type of that test case is still strings (e.g., dicts are json dumped into a string).\r\n\r\nThis ticket is to add a test case where the output of the transform_with_state operation itself contains nested types."}}}
{"metadata": {"id": "13630910", "key": "SPARK-53821", "title": "Ban `org.apache.commons.lang3` package", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T17:38:53.000+0000", "updated": "2025-10-07T17:53:32.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 371\n[https://github.com/apache/spark-kubernetes-operator/pull/371]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53821?", "answer": "No description available."}}}
{"metadata": {"id": "13630901", "key": "SPARK-53820", "title": "Introduce `o.a.s.k8s.operator.utils.StringUtils`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T17:01:46.000+0000", "updated": "2025-10-07T17:13:42.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 370\n[https://github.com/apache/spark-kubernetes-operator/pull/370]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53820?", "answer": "No description available."}}}
{"metadata": {"id": "13630861", "key": "SPARK-53819", "title": "Upgrade commons-lang3 to 3.19.0", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-07T09:58:18.000+0000", "updated": "2025-10-10T09:39:19.000+0000"}, "content": {"description": "Commons-lang3 3.12.0 contains CVE-2025-48924", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53819?", "answer": "Commons-lang3 3.12.0 contains CVE-2025-48924"}}}
{"metadata": {"id": "13630841", "key": "SPARK-53818", "title": "Improve `SentinelManager.toString` by JEP-280 instead of `ToStringBuilder`", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T04:42:30.000+0000", "updated": "2025-10-07T06:17:22.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 369\n[https://github.com/apache/spark-kubernetes-operator/pull/369]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53818?", "answer": "No description available."}}}
{"metadata": {"id": "13630840", "key": "SPARK-53817", "title": "Use `o.a.s.util.Pair` instead of `o.a.commons.lang3.tuple.Pair`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T04:08:07.000+0000", "updated": "2025-10-07T06:16:45.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 368\n[https://github.com/apache/spark-kubernetes-operator/pull/368]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53817?", "answer": "No description available."}}}
{"metadata": {"id": "13630838", "key": "SPARK-53816", "title": "Avoid Nested ExecutionIds with Duplicate Query Plans in Streaming Queries", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Zerui Bao", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-07T03:28:59.000+0000", "updated": "2025-10-07T04:09:18.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53816?", "answer": "No description available."}}}
{"metadata": {"id": "13630833", "key": "SPARK-53815", "title": "Remove `branch-0.4` from daily `publish_snapshot_*` GitHub Action jobs", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T02:10:22.000+0000", "updated": "2025-10-11T19:46:26.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 367\n[https://github.com/apache/spark-kubernetes-operator/pull/367]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53815?", "answer": "No description available."}}}
{"metadata": {"id": "13630832", "key": "SPARK-53814", "title": "Use Java `List.of` instead of `Collections.(empty|singleton)List`", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T01:56:25.000+0000", "updated": "2025-10-07T15:34:50.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 366\n[https://github.com/apache/spark-kubernetes-operator/pull/366]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53814?", "answer": "No description available."}}}
{"metadata": {"id": "13630831", "key": "SPARK-53813", "title": "Recover `Publish Snapshot Chart` GitHub Action Job by using GitHash", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-07T01:25:25.000+0000", "updated": "2025-10-07T01:49:34.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 365\n[https://github.com/apache/spark-kubernetes-operator/pull/365]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53813?", "answer": "No description available."}}}
{"metadata": {"id": "13630827", "key": "SPARK-53812", "title": "Refactor DefineDataset and DefineFlow protos to group related properties and future-proof", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "labels": ["pull-request-available"], "created": "2025-10-06T23:14:22.000+0000", "updated": "2025-10-08T23:09:21.000+0000"}, "content": {"description": "The DefineDataset and DefineFlow Spark Connect protos are moshpits of properties that could be refactored into a more coherent structure:\r\n- In DefineDataset, there are a set of properties that are only relevant to tables (not views). They can be \r\n- In DefineFlow, the relation property refers to flows that write the results of a relation to a target table. In the future, we may want to introduce additional flows types that mutate the target table in different ways.", "comments": ["Issue resolved by pull request 52532\n[https://github.com/apache/spark/pull/52532]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53812?", "answer": "The DefineDataset and DefineFlow Spark Connect protos are moshpits of properties that could be refactored into a more coherent structure:\r\n- In DefineDataset, there are a set of properties that are only relevant to tables (not views). They can be \r\n- In DefineFlow, the relation property refers to flows that write the results of a relation to a target table. In the future, we may want to introduce additional flows types that mutate the target table in different ways."}}}
{"metadata": {"id": "13630815", "key": "SPARK-53811", "title": "HashAggregateCodegenInterruptionSuite can be flaky", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Tim Armstrong", "assignee": null, "labels": [], "created": "2025-10-06T22:29:03.000+0000", "updated": "2025-10-06T22:31:56.000+0000"}, "content": {"description": "```- SPARK-50806: HashAggregate codegen should be interrupted on task cancellation *** FAILED *** (1 minute, 32 seconds)\r\n  The code passed to eventually never returned normally. Attempted 3854 times over 1.0002262633833334 minutes. Last failure message: 3 did not equal 2. (HashAggregateCodegenInterruptionSuite.scala:93)\r\n  org.scalatest.exceptions.TestFailedDueToTimeoutException:```", "comments": ["I can post a fix"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53811?", "answer": "```- SPARK-50806: HashAggregate codegen should be interrupted on task cancellation *** FAILED *** (1 minute, 32 seconds)\r\n  The code passed to eventually never returned normally. Attempted 3854 times over 1.0002262633833334 minutes. Last failure message: 3 did not equal 2. (HashAggregateCodegenInterruptionSuite.scala:93)\r\n  org.scalatest.exceptions.TestFailedDueToTimeoutException:```"}}}
{"metadata": {"id": "13630810", "key": "SPARK-53810", "title": "Split large TWS python tests into multiple small tests to speedup the CI", "project": "SPARK", "status": "Closed", "priority": "Major", "reporter": "Huanli Wang", "assignee": "Huanli Wang", "labels": ["pull-request-available"], "created": "2025-10-06T21:46:40.000+0000", "updated": "2025-10-08T17:40:03.000+0000"}, "content": {"description": "title", "comments": ["PR merged here: https://github.com/apache/spark/pull/52531"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53810?", "answer": "title"}}}
{"metadata": {"id": "13630800", "key": "SPARK-53809", "title": "Add canonicalization for dsv2 scan", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-06T18:02:55.000+0000", "updated": "2025-10-10T23:44:26.000+0000"}, "content": {"description": "Query optimization rules such as MergeScalarSubqueries check if two plans are identical by [comparing their canonicalized form|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala#L219]. For DSv2, for physical plan, the canonicalization goes down in the child hierarchy to the BatchScanExec, which [has a doCanonicalize function|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150]; for logical plan, the canonicalization goes down to the DataSourceV2ScanRelation, which, however, does not have a doCanonicalize function. As a result, two logical plans who are semantically identical are not identified.\r\n\r\nThis PR proposes to add doCanonicalize function for DataSourceV2ScanRelation. The implementation is similar to [the one implemented in BatchScanExec|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150], because they are both the leafNodes of DSv2 logicalPlan and physicalPlan, respectively.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53809?", "answer": "Query optimization rules such as MergeScalarSubqueries check if two plans are identical by [comparing their canonicalized form|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala#L219]. For DSv2, for physical plan, the canonicalization goes down in the child hierarchy to the BatchScanExec, which [has a doCanonicalize function|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150]; for logical plan, the canonicalization goes down to the DataSourceV2ScanRelation, which, however, does not have a doCanonicalize function. As a result, two logical plans who are semantically identical are not identified.\r\n\r\nThis PR proposes to add doCanonicalize function for DataSourceV2ScanRelation. The implementation is similar to [the one implemented in BatchScanExec|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150], because they are both the leafNodes of DSv2 logicalPlan and physicalPlan, respectively."}}}
{"metadata": {"id": "13630767", "key": "SPARK-53808", "title": "Allow to pass optional JVM args to `spark-connect-scala-client`", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "labels": ["pull-request-available"], "created": "2025-10-06T11:41:32.000+0000", "updated": "2025-10-08T13:08:44.000+0000"}, "content": {"description": "Different from other REPL tools like spark-shell, spark-connect-scala-client doesn't support optional JVM args.", "comments": ["Issue resolved by pull request 52526\n[https://github.com/apache/spark/pull/52526]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53808?", "answer": "Different from other REPL tools like spark-shell, spark-connect-scala-client doesn't support optional JVM args."}}}
{"metadata": {"id": "13630745", "key": "SPARK-53807", "title": "Fix a race condition issue between `unlock` and `releaseAllLocksForTask` in `BlockInfoManager for write locks", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2025-10-06T07:44:50.000+0000", "updated": "2025-10-06T07:44:50.000+0000"}, "content": {"description": "When `unlock` and `releaseAllLocksForTask` try to release the same lock for a task, assertion error can happen.\r\nThis issue can be reproduced by adding following test into `BlockInfoManagerSuite`.\r\n{code}\r\n    val blockId = TestBlockId(\"block\")\r\n    assert(blockInfoManager.lockNewBlockForWriting(blockId, newBlockInfo()))\r\n    blockInfoManager.unlock(blockId)\r\n\r\n    // Without the fix the block below almost always fails.\r\n    (0 to 10).foreach { task =>\r\n      withTaskId(task) {\r\n        blockInfoManager.registerTask(task)\r\n\r\n        assert(blockInfoManager.lockForWriting(blockId).isDefined)\r\n\r\n        val future = Future(blockInfoManager.unlock(blockId, Option(task)))\r\n        blockInfoManager.releaseAllLocksForTask(task)\r\n\r\n        ThreadUtils.awaitReady(future, 100.millis)\r\n      }\r\n    }\r\n  }\r\n{code}", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53807?", "answer": "When `unlock` and `releaseAllLocksForTask` try to release the same lock for a task, assertion error can happen.\r\nThis issue can be reproduced by adding following test into `BlockInfoManagerSuite`.\r\n{code}\r\n    val blockId = TestBlockId(\"block\")\r\n    assert(blockInfoManager.lockNewBlockForWriting(blockId, newBlockInfo()))\r\n    blockInfoManager.unlock(blockId)\r\n\r\n    // Without the fix the block below almost always fails.\r\n    (0 to 10).foreach { task =>\r\n      withTaskId(task) {\r\n        blockInfoManager.registerTask(task)\r\n\r\n        assert(blockInfoManager.lockForWriting(blockId).isDefined)\r\n\r\n        val future = Future(blockInfoManager.unlock(blockId, Option(task)))\r\n        blockInfoManager.releaseAllLocksForTask(task)\r\n\r\n        ThreadUtils.awaitReady(future, 100.millis)\r\n      }\r\n    }\r\n  }\r\n{code}"}}}
{"metadata": {"id": "13630735", "key": "SPARK-53806", "title": "Allow empty input on AES decrypt to have error class", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Richard Chen", "assignee": "Richard Chen", "labels": ["pull-request-available"], "created": "2025-10-06T06:20:13.000+0000", "updated": "2025-10-08T15:21:16.000+0000"}, "content": {"description": "previously, if empty input is given to AES decrypt, it would return the error\r\n\r\n\"IllegalArgumentException: Invalid buffer arguments\"\r\n\r\nInstead, it should have a spark error message/error class", "comments": ["Issue resolved by pull request 52523\n[https://github.com/apache/spark/pull/52523]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53806?", "answer": "previously, if empty input is given to AES decrypt, it would return the error\r\n\r\n\"IllegalArgumentException: Invalid buffer arguments\"\r\n\r\nInstead, it should have a spark error message/error class"}}}
{"metadata": {"id": "13630726", "key": "SPARK-53805", "title": "Push Variant into DSv2 scan", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Huaxin Gao", "assignee": "Huaxin Gao", "labels": ["pull-request-available"], "created": "2025-10-06T03:35:53.000+0000", "updated": "2025-10-24T16:19:21.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53805?", "answer": "No description available."}}}
{"metadata": {"id": "13630718", "key": "SPARK-53804", "title": "Support time radix sort", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Bruce Robbins", "assignee": "Bruce Robbins", "labels": ["pull-request-available"], "created": "2025-10-05T20:52:05.000+0000", "updated": "2025-10-07T16:22:58.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52520\n[https://github.com/apache/spark/pull/52520]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53804?", "answer": "No description available."}}}
{"metadata": {"id": "13630706", "key": "SPARK-53803", "title": "Add ArimaRegression for time series forecasting in MLlib", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Anand", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-05T08:45:08.000+0000", "updated": "2025-10-21T02:31:25.000+0000"}, "content": {"description": "The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) algorithm for univariate time series forecasting within the Spark ML pipeline API.\r\n\r\nThis work will include:\r\n- Implementation of ARIMA estimator with parameters (p, d, q)\r\n- A fitted model `ArimaRegressionModel` for prediction\r\n- Parameter support for (p, d, q) accessible from Scala and Python APIs\r\n- PySpark bindings under `pyspark.ml.regression`\r\n- Unit tests in Scala and Python for fit/transform, persistence, and predict\r\n- An example usage added to `examples/ml/ArimaRegressionExample.scala`", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53803?", "answer": "The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) algorithm for univariate time series forecasting within the Spark ML pipeline API.\r\n\r\nThis work will include:\r\n- Implementation of ARIMA estimator with parameters (p, d, q)\r\n- A fitted model `ArimaRegressionModel` for prediction\r\n- Parameter support for (p, d, q) accessible from Scala and Python APIs\r\n- PySpark bindings under `pyspark.ml.regression`\r\n- Unit tests in Scala and Python for fit/transform, persistence, and predict\r\n- An example usage added to `examples/ml/ArimaRegressionExample.scala`"}}}
{"metadata": {"id": "13630687", "key": "SPARK-53802", "title": "Support string values for user-specified schema in SDP tables", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "labels": ["pull-request-available"], "created": "2025-10-04T14:50:19.000+0000", "updated": "2025-10-07T23:00:08.000+0000"}, "content": {"description": " E.g.\r\n\r\n```\r\nfrom pyspark.sql.functions import lit\r\n\r\n@dp.materialized_view(schema=\"id LONG, name STRING\")\r\ndef table_with_string_schema():\r\n    return spark.range(5).withColumn(\"name\", lit(\"test\"))\r\n```", "comments": ["Issue resolved by pull request 52517\n[https://github.com/apache/spark/pull/52517]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53802?", "answer": " E.g.\r\n\r\n```\r\nfrom pyspark.sql.functions import lit\r\n\r\n@dp.materialized_view(schema=\"id LONG, name STRING\")\r\ndef table_with_string_schema():\r\n    return spark.range(5).withColumn(\"name\", lit(\"test\"))\r\n```"}}}
{"metadata": {"id": "13630660", "key": "SPARK-53801", "title": "Enable aggregation pushdown in Data Source V2 streaming", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-03T23:15:54.000+0000", "updated": "2025-10-03T23:15:54.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53801?", "answer": "No description available."}}}
{"metadata": {"id": "13630659", "key": "SPARK-53800", "title": "Enable predicate pushdown in Data Source V2 streaming", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Gengliang Wang", "labels": [], "created": "2025-10-03T23:15:35.000+0000", "updated": "2025-10-03T23:15:35.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53800?", "answer": "No description available."}}}
{"metadata": {"id": "13630658", "key": "SPARK-53799", "title": "Enable column pruning in Data Source V2 streaming", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Gengliang Wang", "labels": ["pull-request-available"], "created": "2025-10-03T23:15:13.000+0000", "updated": "2025-10-10T23:03:14.000+0000"}, "content": {"description": "", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53799?", "answer": "No description available."}}}
{"metadata": {"id": "13630657", "key": "SPARK-53798", "title": "Enable operator pushdown in Data Source V2 streaming", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "labels": [], "created": "2025-10-03T23:02:08.000+0000", "updated": "2025-10-10T23:05:15.000+0000"}, "content": {"description": "This umbrella tracks enabling operator pushdowns in DataSource V2 streaming, bringing it closer to batch parity. Supported pushdowns (filters, projections, aggregates, etc) will be applied directly in DSv2 readers during stream analysis to reduce scanned data and compute cost.", "comments": ["Because Spark’s micro-batch streaming must materialize the MicroBatchStream during the analysis phase, and DSv2 constructs it through the sequence ScanBuilder → Scan → Scan.toMicroBatchStream, the optimizer rule V2ScanRelationPushDown needs to be applied early—specifically on the analyzed plan of MicroBatchExecution.\r\n\r\nThis makes the flow somewhat tricky. Moreover, since V2ScanRelationPushDown {*}expects all predicates to be fully combined and pushed down{*}, applying it too early may cause the pushdown to fail. We also need to handle streaming deduplication properly.\r\n\r\nUntil we find a cleaner solution, I’ll make this Jira as on hold to unblock Spark 4.1 release"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53798?", "answer": "This umbrella tracks enabling operator pushdowns in DataSource V2 streaming, bringing it closer to batch parity. Supported pushdowns (filters, projections, aggregates, etc) will be applied directly in DSv2 readers during stream analysis to reduce scanned data and compute cost."}}}
{"metadata": {"id": "13630651", "key": "SPARK-53797", "title": "File stream source maxBytesPerTrigger is incredibly slow", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Adam Binford", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-03T18:30:17.000+0000", "updated": "2025-10-03T18:37:10.000+0000"}, "content": {"description": "Using the new maxBytesPerTrigger setting for file stream sources is incredibly slow in building the batch for sources with a large number of files", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53797?", "answer": "Using the new maxBytesPerTrigger setting for file stream sources is incredibly slow in building the batch for sources with a large number of files"}}}
{"metadata": {"id": "13630649", "key": "SPARK-53796", "title": "Add extension field to pipeline protos to support forward compatibility", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yuheng Chang", "assignee": "Yuheng Chang", "labels": ["pull-request-available"], "created": "2025-10-03T18:23:30.000+0000", "updated": "2025-10-11T15:05:48.000+0000"}, "content": {"description": "Adding {{google.protobuf.Any extension = 999;}} field to {{{}PipelineCommand{}}}, {{{}DefineDataset{}}}, and {{DefineFlow}} Protos to support forward-compatibility by carrying additional pipeline command types / dataset or flow's fields that are not yet defined in this version of the proto.\r\n\r\nDuring the planning stage, the Spark Server will resolve and dispatch command / message to the correct handler.", "comments": ["Issue resolved by pull request 52514\n[https://github.com/apache/spark/pull/52514]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53796?", "answer": "Adding {{google.protobuf.Any extension = 999;}} field to {{{}PipelineCommand{}}}, {{{}DefineDataset{}}}, and {{DefineFlow}} Protos to support forward-compatibility by carrying additional pipeline command types / dataset or flow's fields that are not yet defined in this version of the proto.\r\n\r\nDuring the planning stage, the Spark Server will resolve and dispatch command / message to the correct handler."}}}
{"metadata": {"id": "13630613", "key": "SPARK-53795", "title": "Remove unused parameters in LiteralValueProtoConverter", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Yihong He", "assignee": "Yihong He", "labels": ["pull-request-available"], "created": "2025-10-03T11:47:16.000+0000", "updated": "2025-10-08T02:04:42.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52512\n[https://github.com/apache/spark/pull/52512]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53795?", "answer": "No description available."}}}
{"metadata": {"id": "13630580", "key": "SPARK-53794", "title": "Add support for maxVersionsToDelete while deleting old rocksdb versions", "project": "SPARK", "status": "Closed", "priority": "Major", "reporter": "Anish Shrigondekar", "assignee": "Anish Shrigondekar", "labels": ["pull-request-available"], "created": "2025-10-02T23:08:53.000+0000", "updated": "2025-10-07T04:14:43.000+0000"}, "content": {"description": "Add support for maxVersionsToDelete while deleting old rocksdb", "comments": ["Merged here: https://github.com/apache/spark/pull/52511"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53794?", "answer": "Add support for maxVersionsToDelete while deleting old rocksdb"}}}
{"metadata": {"id": "13630567", "key": "SPARK-53793", "title": "Use DSv2 predicate to evaluate InternalRow", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-02T19:58:20.000+0000", "updated": "2025-10-10T17:32:54.000+0000"}, "content": {"description": "This ticket proposes to add a utility class to enable the evaluation of an InternalRow using a DSv2 predicate. This would be helpful for partition pruning, where the [runtime filters are DSv2 predicates|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsRuntimeV2Filtering.java#L66] and the partitionValue are internalRows (for [partitionFiles in Spark|https://github.com/apache/spark/blob/65ff85a31fe8a8ea4a2ba713ba2c624709ce815a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala#L58]). In this way, partitionFiles can be pruned directly with DSv2 predicates at the scan level. \r\n\r\n \r\n\r\nTo enable this, a DSv2 predicate will be converted to a catalyst expression, and then create an evaluator.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53793?", "answer": "This ticket proposes to add a utility class to enable the evaluation of an InternalRow using a DSv2 predicate. This would be helpful for partition pruning, where the [runtime filters are DSv2 predicates|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsRuntimeV2Filtering.java#L66] and the partitionValue are internalRows (for [partitionFiles in Spark|https://github.com/apache/spark/blob/65ff85a31fe8a8ea4a2ba713ba2c624709ce815a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala#L58]). In this way, partitionFiles can be pruned directly with DSv2 predicates at the scan level. \r\n\r\n \r\n\r\nTo enable this, a DSv2 predicate will be converted to a catalyst expression, and then create an evaluator."}}}
{"metadata": {"id": "13630560", "key": "SPARK-53792", "title": "Fix rocksdbPinnedBlocksMemoryUsage when bounded memory usage is enabled", "project": "SPARK", "status": "Closed", "priority": "Minor", "reporter": "Zifei Feng", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-02T18:06:18.000+0000", "updated": "2025-10-08T17:51:40.000+0000"}, "content": {"description": "We forgot to fix this to show the correct metric when bounded memory usage is enabled. Currently, it is collecting data for each RocksDB without accounting for when all the RocksDB instances on the executor are sharing the same cache, leading to double counting.\r\n\r\nThis is where we collect the cache memory metric: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L2045]\r\n\r\nWe are currently using getDBProperty(\"rocksdb.block-cache-pinned-usage\") to read the pinned blocks for each DB. When the block cache is shared, this is wrong because:\r\n * {*}Instance-specific vs. global stats{*}: Database properties like \"rocksdb.block-cache-pinned-usage\" report on the memory size of entries requested specifically by that DB instance.\r\n * {*}Double-counting potential{*}: If you query the DB property on both instances and add them together, you could potentially double-count because a single block in the shared cache could be used by both DB instances.\r\n\r\n*Solution:* Do lrucache.getPinnedUsage ([https://github.com/facebook/rocksdb/blob/v9.8.4/java/src/main/java/org/rocksdb/Cache.java#L33C15-L33C29] in OSS RocksDB) instead, to get the actual memory size of pinned blocks in the shared cache. We are querying the cache here instead of the DB.\r\n\r\nTo estimate the pinnedBlocks used by a single DB instance, we can divide lrucache.getPinnedUsage() by num_rocksdb_instances_sharing_the_cache. We already do a similar thing for memoryUsage. See: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBMemoryManager.scala#L124]\r\n\r\n*Original fix:* OSS Spark PR:[ https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2|https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2]", "comments": ["PR merged here: https://github.com/apache/spark/pull/52527"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53792?", "answer": "We forgot to fix this to show the correct metric when bounded memory usage is enabled. Currently, it is collecting data for each RocksDB without accounting for when all the RocksDB instances on the executor are sharing the same cache, leading to double counting.\r\n\r\nThis is where we collect the cache memory metric: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L2045]\r\n\r\nWe are currently using getDBProperty(\"rocksdb.block-cache-pinned-usage\") to read the pinned blocks for each DB. When the block cache is shared, this is wrong because:\r\n * {*}Instance-specific vs. global stats{*}: Database properties like \"rocksdb.block-cache-pinned-usage\" report on the memory size of entries requested specifically by that DB instance.\r\n * {*}Double-counting potential{*}: If you query the DB property on both instances and add them together, you could potentially double-count because a single block in the shared cache could be used by both DB instances.\r\n\r\n*Solution:* Do lrucache.getPinnedUsage ([https://github.com/facebook/rocksdb/blob/v9.8.4/java/src/main/java/org/rocksdb/Cache.java#L33C15-L33C29] in OSS RocksDB) instead, to get the actual memory size of pinned blocks in the shared cache. We are querying the cache here instead of the DB.\r\n\r\nTo estimate the pinnedBlocks used by a single DB instance, we can divide lrucache.getPinnedUsage() by num_rocksdb_instances_sharing_the_cache. We already do a similar thing for memoryUsage. See: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBMemoryManager.scala#L124]\r\n\r\n*Original fix:* OSS Spark PR:[ https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2|https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2]"}}}
{"metadata": {"id": "13630554", "key": "SPARK-53791", "title": "Make the rename operations multi-threaded.", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "duanshilong", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-02T15:15:16.000+0000", "updated": "2025-10-15T03:05:46.000+0000"}, "content": {"description": "For example, during the INSERT OVERWRITE DIRECTORY operation, each rename operation triggers an RPC request. Therefore, when there are too many files, it can be time-consuming.\r\nConverting the serial rename operations to multi-threaded operations can save job execution time.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53791?", "answer": "For example, during the INSERT OVERWRITE DIRECTORY operation, each rename operation triggers an RPC request. Therefore, when there are too many files, it can be time-consuming.\r\nConverting the serial rename operations to multi-threaded operations can save job execution time."}}}
{"metadata": {"id": "13630523", "key": "SPARK-53790", "title": "arrow encoder is unable to handle Beans with instantiated generics", "project": "SPARK", "status": "Open", "priority": "Major", "reporter": "Steven Aerts", "assignee": null, "labels": ["pull-request-available"], "created": "2025-10-02T11:55:46.000+0000", "updated": "2025-10-03T14:32:43.000+0000"}, "content": {"description": "We found a bug in the arrow encoder/decoder for spark connect which is unable to handle beans with instantiated generics.  Like:\r\n```\r\nclass JavaBeanWithGenerics[T] {\r\n  @BeanProperty var value: T = _\r\n}\r\nclass JavaBeanWithGenericsWrapper {\r\n  @BeanProperty var value: JavaBeanWithGenerics[String] = _\r\n}\r\n...\r\nval encoder = JavaTypeInference.encoderFor(classOf[JavaBeanWithGenericsWrapper])\r\n```\r\n\r\nWhich results in the following error when the above encoder is used in connect:\r\n```\r\njava.lang.NoSuchMethodException: no such method: JavaBeanWithGenerics.getValue()String/invokeVirtual\r\nat java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:915)\r\nat java.base/java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:994)\r\nat java.base/java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:3750)\r\nat java.base/java.lang.invoke.MethodHandles$Lookup.findVirtual(MethodHandles.java:2767)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19(ArrowSerializer.scala:495)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19$adapted(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:549)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:548)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:237)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer.<init>(ArrowSerializer.scala:62)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$$anon$1.<init>(ArrowSerializer.scala:161)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serialize(ArrowSerializer.scala:160)\r\n```\r\n\r\nWe have have a possible patch for this issue.", "comments": []}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53790?", "answer": "We found a bug in the arrow encoder/decoder for spark connect which is unable to handle beans with instantiated generics.  Like:\r\n```\r\nclass JavaBeanWithGenerics[T] {\r\n  @BeanProperty var value: T = _\r\n}\r\nclass JavaBeanWithGenericsWrapper {\r\n  @BeanProperty var value: JavaBeanWithGenerics[String] = _\r\n}\r\n...\r\nval encoder = JavaTypeInference.encoderFor(classOf[JavaBeanWithGenericsWrapper])\r\n```\r\n\r\nWhich results in the following error when the above encoder is used in connect:\r\n```\r\njava.lang.NoSuchMethodException: no such method: JavaBeanWithGenerics.getValue()String/invokeVirtual\r\nat java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:915)\r\nat java.base/java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:994)\r\nat java.base/java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:3750)\r\nat java.base/java.lang.invoke.MethodHandles$Lookup.findVirtual(MethodHandles.java:2767)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19(ArrowSerializer.scala:495)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19$adapted(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:549)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:548)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r\nat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:237)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer.<init>(ArrowSerializer.scala:62)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$$anon$1.<init>(ArrowSerializer.scala:161)\r\nat org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serialize(ArrowSerializer.scala:160)\r\n```\r\n\r\nWe have have a possible patch for this issue."}}}
{"metadata": {"id": "13630503", "key": "SPARK-53789", "title": "Canonicalize error condition CANNOT_MODIFY_STATIC_CONFIG", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-02T06:51:41.000+0000", "updated": "2025-10-15T04:34:57.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52506\n[https://github.com/apache/spark/pull/52506]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53789?", "answer": "No description available."}}}
{"metadata": {"id": "13630502", "key": "SPARK-53788", "title": "Move VersionUtils to common utils", "project": "SPARK", "status": "Resolved", "priority": "Minor", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "labels": ["pull-request-available"], "created": "2025-10-02T06:29:37.000+0000", "updated": "2025-10-07T19:04:06.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52505\n[https://github.com/apache/spark/pull/52505]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53788?", "answer": "No description available."}}}
{"metadata": {"id": "13630501", "key": "SPARK-53787", "title": "Upgrade Spark to 4.1.0-preview2", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-02T06:26:42.000+0000", "updated": "2025-10-02T06:38:35.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 364\n[https://github.com/apache/spark-kubernetes-operator/pull/364]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53787?", "answer": "No description available."}}}
{"metadata": {"id": "13630500", "key": "SPARK-53786", "title": "Default value should not conflict with special column name", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Szehon Ho", "assignee": "Szehon Ho", "labels": ["pull-request-available"], "created": "2025-10-02T06:06:46.000+0000", "updated": "2025-10-10T11:03:40.000+0000"}, "content": {"description": "The following query:\r\n\r\nCREATE TABLE t (current_timestamp DEFAULT current_timestamp)\r\n\r\n \r\n\r\nfails after SPARK-51786.  This is because for making default value DSV2 expression, it uses the main analyzer to analyze the default value.  However, default value should not know about other columns when trying to analyze.", "comments": ["Issue resolved by pull request 52504\n[https://github.com/apache/spark/pull/52504]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53786?", "answer": "The following query:\r\n\r\nCREATE TABLE t (current_timestamp DEFAULT current_timestamp)\r\n\r\n \r\n\r\nfails after SPARK-51786.  This is because for making default value DSV2 expression, it uses the main analyzer to analyze the default value.  However, default value should not know about other columns when trying to analyze."}}}
{"metadata": {"id": "13630499", "key": "SPARK-53785", "title": "Memory Source for RTM", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "labels": ["pull-request-available"], "created": "2025-10-02T05:59:55.000+0000", "updated": "2025-10-24T05:11:23.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 52502\n[https://github.com/apache/spark/pull/52502]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53785?", "answer": "No description available."}}}
{"metadata": {"id": "13630495", "key": "SPARK-53784", "title": "Additional Source APIs needed to support RTM execution", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "labels": ["pull-request-available"], "created": "2025-10-02T04:57:51.000+0000", "updated": "2025-10-24T05:11:31.000+0000"}, "content": {"description": "Currently in Structured Streaming, start and end offsets are determined at the driver prior to running the micro-batch.  In real-time mode, end offsets are not known apriori. They are communicated to the driver later by the executors at the end of microbatch that runs for a fixed amount of time.  Thus, we need to add additional APIs in the source to support this kind of behavior.\r\n\r\nThe lifecycle of the new API is the following.\r\n\r\nDriver side:\r\n # prepareForRealTimeMode\r\n ** Called during logical planning to inform the source if it's in real time mode\r\n # planInputPartitions\r\n ** The driver plans partitions via planPartitions but only a starting offset is provided (Compared to existing execution modes that require planPartitions to provide both a starting and end offset)\r\n # mergeOffsets\r\n ** Merge partitioned offsets coming from partitions/tasks to a single global offset.\r\n\r\n \r\n\r\nTask side:\r\n # nextWithTimeout\r\n ** Alternative function to be called than next(), that proceed to the next record. The different from next() is that, if there is no more records, the call needs to keep waiting until the timeout\r\n # \r\ngetOffset\r\n ** Get the offset of the next record, or the start offset if no records have been read. The execution engine will call this method along with get() to keep track of the current offset. When a task ends, the offset in each partition will be passed back to the driver. They will be used as the start offsets of the next batch.\r\n\r\n ", "comments": ["Issue resolved by pull request 52501\n[https://github.com/apache/spark/pull/52501]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53784?", "answer": "Currently in Structured Streaming, start and end offsets are determined at the driver prior to running the micro-batch.  In real-time mode, end offsets are not known apriori. They are communicated to the driver later by the executors at the end of microbatch that runs for a fixed amount of time.  Thus, we need to add additional APIs in the source to support this kind of behavior.\r\n\r\nThe lifecycle of the new API is the following.\r\n\r\nDriver side:\r\n # prepareForRealTimeMode\r\n ** Called during logical planning to inform the source if it's in real time mode\r\n # planInputPartitions\r\n ** The driver plans partitions via planPartitions but only a starting offset is provided (Compared to existing execution modes that require planPartitions to provide both a starting and end offset)\r\n # mergeOffsets\r\n ** Merge partitioned offsets coming from partitions/tasks to a single global offset.\r\n\r\n \r\n\r\nTask side:\r\n # nextWithTimeout\r\n ** Alternative function to be called than next(), that proceed to the next record. The different from next() is that, if there is no more records, the call needs to keep waiting until the timeout\r\n # \r\ngetOffset\r\n ** Get the offset of the next record, or the start offset if no records have been read. The execution engine will call this method along with get() to keep track of the current offset. When a task ends, the offset in each partition will be passed back to the driver. They will be used as the start offsets of the next batch.\r\n\r\n "}}}
{"metadata": {"id": "13630494", "key": "SPARK-53783", "title": "Use `log4j2` instead of `log4j`", "project": "SPARK", "status": "Resolved", "priority": "Critical", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-02T04:11:15.000+0000", "updated": "2025-10-02T04:47:41.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 363\n[https://github.com/apache/spark-kubernetes-operator/pull/363]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53783?", "answer": "No description available."}}}
{"metadata": {"id": "13630493", "key": "SPARK-53782", "title": "Use `slf4j-api` as a direct dependency", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-02T03:26:10.000+0000", "updated": "2025-10-02T03:42:09.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 362\n[https://github.com/apache/spark-kubernetes-operator/pull/362]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53782?", "answer": "No description available."}}}
{"metadata": {"id": "13630489", "key": "SPARK-53781", "title": "Exclude Spark's transitive dependencies consistently across modules", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-02T00:17:20.000+0000", "updated": "2025-10-02T06:23:54.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 361\n[https://github.com/apache/spark-kubernetes-operator/pull/361]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53781?", "answer": "No description available."}}}
{"metadata": {"id": "13630482", "key": "SPARK-53780", "title": "`spark-submission-worker` should depend on `io.fabric8` directly", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-01T22:39:09.000+0000", "updated": "2025-10-02T00:14:26.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 360\n[https://github.com/apache/spark-kubernetes-operator/pull/360]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53780?", "answer": "No description available."}}}
{"metadata": {"id": "13630479", "key": "SPARK-53779", "title": "Implement transform in column API", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Yicong Huang", "assignee": "Yicong Huang", "labels": ["pull-request-available"], "created": "2025-10-01T21:42:17.000+0000", "updated": "2025-10-10T10:58:56.000+0000"}, "content": {"description": "Proposal to introduce a transform API for Column in Spark, inspired by Scala’s pipe operator and SQL pipeline syntax. This would allow chaining transformations in a pipeline style, improving readability compared to nested function calls.\r\n\r\n \r\n\r\n*Motivation*\r\n * Scala’s pipe API and SQL pipeline syntax provide a cleaner, pipeline-oriented style.\r\n\r\n * Current nested function invocations (e.g., f2(f1(col))) are less readable than a chained style (col.transform(f1).transform(f2)).\r\n\r\n * AI code generators also tend to produce pipeline style code more cleanly.\r\n\r\n * This aligns with the existing DataFrame API pipeline style (df.transform(f) → DataFrame).\r\n\r\n ", "comments": ["I will work on this.", "PR opened at https://github.com/apache/spark/pull/52537", "Issue resolved by pull request 52537\n[https://github.com/apache/spark/pull/52537]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53779?", "answer": "Proposal to introduce a transform API for Column in Spark, inspired by Scala’s pipe operator and SQL pipeline syntax. This would allow chaining transformations in a pipeline style, improving readability compared to nested function calls.\r\n\r\n \r\n\r\n*Motivation*\r\n * Scala’s pipe API and SQL pipeline syntax provide a cleaner, pipeline-oriented style.\r\n\r\n * Current nested function invocations (e.g., f2(f1(col))) are less readable than a chained style (col.transform(f1).transform(f2)).\r\n\r\n * AI code generators also tend to produce pipeline style code more cleanly.\r\n\r\n * This aligns with the existing DataFrame API pipeline style (df.transform(f) → DataFrame).\r\n\r\n "}}}
{"metadata": {"id": "13630477", "key": "SPARK-53778", "title": "Use JDK25 Gradle Image as builder images", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-01T20:37:40.000+0000", "updated": "2025-10-01T20:54:59.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 359\n[https://github.com/apache/spark-kubernetes-operator/pull/359]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53778?", "answer": "No description available."}}}
{"metadata": {"id": "13630470", "key": "SPARK-53777", "title": "Update `Spark Connect`-generated `Swift` source code with 4.1.0-preview2", "project": "SPARK", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "labels": ["pull-request-available"], "created": "2025-10-01T19:09:16.000+0000", "updated": "2025-10-01T20:07:15.000+0000"}, "content": {"description": "", "comments": ["Issue resolved by pull request 250\n[https://github.com/apache/spark-connect-swift/pull/250]"]}, "derived_tasks": {"summarization": "Summarize the issue and its discussion.", "classification": "Classify the issue as bug, improvement, or feature.", "qna": {"question": "What is the main problem discussed in issue SPARK-53777?", "answer": "No description available."}}}
